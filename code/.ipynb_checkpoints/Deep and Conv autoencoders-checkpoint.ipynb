{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.layers import Input, Dense, Conv2D, MaxPooling2D, UpSampling2D\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "import pickle\n",
    "import random as rand\n",
    "import time\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# globals\n",
    "NUMBER_OF_TRAINING_INSTANCES = 2\n",
    "CHANNELS = 1\n",
    "TIMESTAMPS = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# random signal loader, for loading toy data\n",
    "def random_signal_loader(static_indices, shuffled_non_static_indices, file_path):\n",
    "    training_instance = False\n",
    "    \n",
    "    if(len(shuffled_non_static_indices) == 0):\n",
    "        shuffled_non_static_indices = list(static_indices)\n",
    "        rand.shuffle(shuffled_non_static_indices)\n",
    "    with open(file_path, \"rb\") as input_file:\n",
    "        input_file.seek(shuffled_non_static_indices.pop(0))\n",
    "        training_instance = pickle.load(input_file)\n",
    "        \n",
    "    return training_instance, shuffled_non_static_indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test and training set size selector, for cropping the data\n",
    "def set_creator(input_size, data_set):\n",
    "    size = data_set.shape[0]\n",
    "    reduce = size % input_size\n",
    "    if reduce != 0:\n",
    "        return data_set[:-reduce,:]\n",
    "    else:\n",
    "        return data_set\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tlpel\\Documents\\Studie\\AI1\\pAI\\code\\unsupervised data\\\n",
      "(78000, 36)\n",
      "(80000, 36)\n",
      "(79000, 36)\n",
      "(80000, 36)\n",
      "(77000, 36)\n",
      "(191000, 36)\n",
      "(269000, 36)\n",
      "(460000, 36)\n"
     ]
    }
   ],
   "source": [
    "# Load the unsupervised data from folder, make sure the folder name is unsupervised data and in your path\n",
    "path = os.getcwd() + '\\\\unsupervised data\\\\'\n",
    "print(path)\n",
    "for file in os.listdir(path):\n",
    "    if file.endswith(\".CSV\"):\n",
    "        data = pd.read_csv(path + file)\n",
    "        print(data.shape)\n",
    "        dataset.append(data)\n",
    "    '''if file.endswith(\".edf\"):\n",
    "        print(path+file)\n",
    "        reader = pyedflib.EdfReader(path+file)\n",
    "        print(reader)'''\n",
    "\n",
    "# Get one long data file\n",
    "data = pd.concat(dataset, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1314000, 36)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COUNTER</th>\n",
       "      <th>INTERPOLATED</th>\n",
       "      <th>AF3</th>\n",
       "      <th>F7</th>\n",
       "      <th>F3</th>\n",
       "      <th>FC5</th>\n",
       "      <th>T7</th>\n",
       "      <th>P7</th>\n",
       "      <th>O1</th>\n",
       "      <th>O2</th>\n",
       "      <th>...</th>\n",
       "      <th>CQ_T8</th>\n",
       "      <th>CQ_FC6</th>\n",
       "      <th>CQ_F4</th>\n",
       "      <th>CQ_F8</th>\n",
       "      <th>CQ_AF4</th>\n",
       "      <th>CQ_CMS</th>\n",
       "      <th>CQ_DRL</th>\n",
       "      <th>GYROX</th>\n",
       "      <th>GYROY</th>\n",
       "      <th>MARKER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>53.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4622.563990</td>\n",
       "      <td>4075.384516</td>\n",
       "      <td>4276.922972</td>\n",
       "      <td>4031.281953</td>\n",
       "      <td>4685.128091</td>\n",
       "      <td>4216.922974</td>\n",
       "      <td>4779.487063</td>\n",
       "      <td>4410.256302</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1672.0</td>\n",
       "      <td>1679.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>54.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4617.435785</td>\n",
       "      <td>4076.410157</td>\n",
       "      <td>4275.384511</td>\n",
       "      <td>4031.281953</td>\n",
       "      <td>4687.692193</td>\n",
       "      <td>4217.435794</td>\n",
       "      <td>4779.487063</td>\n",
       "      <td>4413.333225</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1672.0</td>\n",
       "      <td>1679.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>55.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4616.410144</td>\n",
       "      <td>4078.461439</td>\n",
       "      <td>4275.384511</td>\n",
       "      <td>4034.358876</td>\n",
       "      <td>4688.205014</td>\n",
       "      <td>4221.025538</td>\n",
       "      <td>4785.128088</td>\n",
       "      <td>4412.307584</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1670.0</td>\n",
       "      <td>1680.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4617.948605</td>\n",
       "      <td>4082.564003</td>\n",
       "      <td>4274.358870</td>\n",
       "      <td>4037.435799</td>\n",
       "      <td>4688.717834</td>\n",
       "      <td>4220.512717</td>\n",
       "      <td>4784.102447</td>\n",
       "      <td>4412.307584</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1670.0</td>\n",
       "      <td>1682.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>57.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4618.974246</td>\n",
       "      <td>4088.717849</td>\n",
       "      <td>4274.358870</td>\n",
       "      <td>4035.897337</td>\n",
       "      <td>4688.205014</td>\n",
       "      <td>4218.974256</td>\n",
       "      <td>4781.538345</td>\n",
       "      <td>4414.871687</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1669.0</td>\n",
       "      <td>1683.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>58.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4619.487067</td>\n",
       "      <td>4085.640926</td>\n",
       "      <td>4272.820408</td>\n",
       "      <td>4032.820414</td>\n",
       "      <td>4684.102450</td>\n",
       "      <td>4217.435794</td>\n",
       "      <td>4781.538345</td>\n",
       "      <td>4413.333225</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1669.0</td>\n",
       "      <td>1686.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>59.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4611.794759</td>\n",
       "      <td>4077.435798</td>\n",
       "      <td>4265.640921</td>\n",
       "      <td>4028.717850</td>\n",
       "      <td>4679.999886</td>\n",
       "      <td>4212.820410</td>\n",
       "      <td>4772.820396</td>\n",
       "      <td>4404.102456</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1670.0</td>\n",
       "      <td>1685.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>60.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4603.076811</td>\n",
       "      <td>4077.948618</td>\n",
       "      <td>4261.025537</td>\n",
       "      <td>4026.666568</td>\n",
       "      <td>4678.974245</td>\n",
       "      <td>4211.281948</td>\n",
       "      <td>4765.128089</td>\n",
       "      <td>4404.615277</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1668.0</td>\n",
       "      <td>1687.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>61.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4599.999888</td>\n",
       "      <td>4079.487080</td>\n",
       "      <td>4262.563998</td>\n",
       "      <td>4029.743491</td>\n",
       "      <td>4680.512706</td>\n",
       "      <td>4214.871692</td>\n",
       "      <td>4769.743473</td>\n",
       "      <td>4408.717841</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1670.0</td>\n",
       "      <td>1686.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>62.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4601.538349</td>\n",
       "      <td>4074.871695</td>\n",
       "      <td>4263.589639</td>\n",
       "      <td>4032.820414</td>\n",
       "      <td>4683.589629</td>\n",
       "      <td>4216.922974</td>\n",
       "      <td>4771.794755</td>\n",
       "      <td>4402.051174</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1669.0</td>\n",
       "      <td>1686.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   COUNTER   INTERPOLATED          AF3           F7           F3          FC5  \\\n",
       "0     53.0            0.0  4622.563990  4075.384516  4276.922972  4031.281953   \n",
       "1     54.0            0.0  4617.435785  4076.410157  4275.384511  4031.281953   \n",
       "2     55.0            0.0  4616.410144  4078.461439  4275.384511  4034.358876   \n",
       "3     56.0            0.0  4617.948605  4082.564003  4274.358870  4037.435799   \n",
       "4     57.0            0.0  4618.974246  4088.717849  4274.358870  4035.897337   \n",
       "5     58.0            0.0  4619.487067  4085.640926  4272.820408  4032.820414   \n",
       "6     59.0            0.0  4611.794759  4077.435798  4265.640921  4028.717850   \n",
       "7     60.0            0.0  4603.076811  4077.948618  4261.025537  4026.666568   \n",
       "8     61.0            0.0  4599.999888  4079.487080  4262.563998  4029.743491   \n",
       "9     62.0            0.0  4601.538349  4074.871695  4263.589639  4032.820414   \n",
       "\n",
       "            T7           P7           O1           O2   ...      CQ_T8  \\\n",
       "0  4685.128091  4216.922974  4779.487063  4410.256302   ...        4.0   \n",
       "1  4687.692193  4217.435794  4779.487063  4413.333225   ...        4.0   \n",
       "2  4688.205014  4221.025538  4785.128088  4412.307584   ...        4.0   \n",
       "3  4688.717834  4220.512717  4784.102447  4412.307584   ...        4.0   \n",
       "4  4688.205014  4218.974256  4781.538345  4414.871687   ...        4.0   \n",
       "5  4684.102450  4217.435794  4781.538345  4413.333225   ...        4.0   \n",
       "6  4679.999886  4212.820410  4772.820396  4404.102456   ...        4.0   \n",
       "7  4678.974245  4211.281948  4765.128089  4404.615277   ...        4.0   \n",
       "8  4680.512706  4214.871692  4769.743473  4408.717841   ...        4.0   \n",
       "9  4683.589629  4216.922974  4771.794755  4402.051174   ...        4.0   \n",
       "\n",
       "    CQ_FC6   CQ_F4   CQ_F8   CQ_AF4   CQ_CMS   CQ_DRL   GYROX   GYROY   MARKER  \n",
       "0      4.0     4.0     4.0      4.0      4.0      4.0  1672.0  1679.0      0.0  \n",
       "1      4.0     4.0     4.0      4.0      4.0      4.0  1672.0  1679.0      0.0  \n",
       "2      4.0     4.0     4.0      4.0      4.0      4.0  1670.0  1680.0      0.0  \n",
       "3      4.0     4.0     4.0      4.0      4.0      4.0  1670.0  1682.0      0.0  \n",
       "4      4.0     4.0     4.0      4.0      4.0      4.0  1669.0  1683.0      0.0  \n",
       "5      4.0     4.0     4.0      4.0      4.0      4.0  1669.0  1686.0      0.0  \n",
       "6      4.0     4.0     4.0      4.0      4.0      4.0  1670.0  1685.0      0.0  \n",
       "7      4.0     4.0     4.0      4.0      4.0      4.0  1668.0  1687.0      0.0  \n",
       "8      4.0     4.0     4.0      4.0      4.0      4.0  1670.0  1686.0      0.0  \n",
       "9      4.0     4.0     4.0      4.0      4.0      4.0  1669.0  1686.0      0.0  \n",
       "\n",
       "[10 rows x 36 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check whether everything looks okay, then proceed\n",
    "print(data.shape)\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['COUNTER', ' INTERPOLATED', ' AF3', ' F7', ' F3', ' FC5', ' T7', ' P7',\n",
       "       ' O1', ' O2', ' P8', ' T8', ' FC6', ' F4', ' F8', ' AF4', ' RAW_CQ',\n",
       "       ' CQ_AF3', ' CQ_F7', ' CQ_F3', ' CQ_FC5', ' CQ_T7', ' CQ_P7', ' CQ_O1',\n",
       "       ' CQ_O2', ' CQ_P8', ' CQ_T8', ' CQ_FC6', ' CQ_F4', ' CQ_F8', ' CQ_AF4',\n",
       "       ' CQ_CMS', ' CQ_DRL', ' GYROX', ' GYROY', ' MARKER'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Deep Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((563640, 14), (290360, 14), numpy.ndarray, 563640, 290360)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Do a 66/34 split of training and test. Play a bit with this to get nice sized training and test sets.\n",
    "n_train = int(data.shape[0]*0.66) \n",
    "n_test = data.shape[0] - n_train\n",
    "\n",
    "x_train = data.loc[:n_train-1, ' AF3':' AF4'].values\n",
    "x_test = data.loc[n_train:, ' AF3':' AF4'].values\n",
    "\n",
    "x_train.shape, x_test.shape, type(x_train), n_train, n_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Deep autoencoder\n",
    "datapoint_dim = [1, CHANNELS * TIMESTAMPS]  # dimensionality of one datapoint\n",
    "input_dim = CHANNELS * TIMESTAMPS # 28 timepoints of 14 channels (about 0.23 seconds of data)\n",
    "hidden_dim = 64  # For 280-dimensional input data (20*14) these layer sizes seem to suffice. Play with this if you change the input.\n",
    "encoding_dim = 32\n",
    "\n",
    "# this is our input placeholder\n",
    "input_eeg = Input(shape=(input_dim,))\n",
    "\n",
    "# \"encoded\" is the encoded representation of the input\n",
    "encoded = Dense(hidden_dim, activation=\"relu\")(input_eeg)\n",
    "encoded = Dense(encoding_dim, activation='relu')(encoded)\n",
    "\n",
    "# \"decoded\" is the lossy reconstruction of the input\n",
    "decoded = Dense(hidden_dim, activation='relu')(encoded)\n",
    "decoded = Dense(input_dim, activation='sigmoid')(decoded)\n",
    "\n",
    "# this model maps an input to its reconstruction\n",
    "autoencoder = Model(input_eeg, decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# this model maps an input to its encoded representation\n",
    "encoder = Model(input_eeg, encoded)\n",
    "\n",
    "# this model maps an encoded representation to a reconstruction\n",
    "# decoder = Model(encoded, decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create a placeholder for an encoded input\n",
    "encoded_input = Input(shape=(encoding_dim,))\n",
    "\n",
    "# retrieve the decoder layers of the autoencoder model\n",
    "decoder_layer = autoencoder.layers[-2](encoded_input)\n",
    "decoder_layer = autoencoder.layers[-1](decoder_layer)\n",
    "\n",
    "# create the decoder model\n",
    "decoder = Model(encoded_input, decoder_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "autoencoder.compile(optimizer='adam', loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((28182, 280), (14518, 280))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We will normalize all values between 0 and 1 and we will flatten the 28x28 images into vectors of size 784.\n",
    "\n",
    "normalize = max(np.amax(x_train), np.amax(x_test))\n",
    "\n",
    "x_train = x_train.astype('float32') / normalize\n",
    "x_test = x_test.astype('float32') / normalize\n",
    "x_train = x_train.reshape(int(n_train/20.0), 14*20)\n",
    "x_test = x_test.reshape(int(n_test/20.0), 14*20)\n",
    "x_train.shape, x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "28182/28182 [==============================] - 8s - loss: 2.9815e-04     \n",
      "Epoch 2/50\n",
      "28182/28182 [==============================] - 2s - loss: 1.6537e-04     \n",
      "Epoch 3/50\n",
      "28182/28182 [==============================] - 5s - loss: 1.4896e-04     \n",
      "Epoch 4/50\n",
      "28182/28182 [==============================] - 1s - loss: 1.3800e-04     \n",
      "Epoch 5/50\n",
      "28182/28182 [==============================] - 2s - loss: 1.2190e-04     \n",
      "Epoch 6/50\n",
      "28182/28182 [==============================] - 5s - loss: 1.1251e-04     \n",
      "Epoch 7/50\n",
      "28182/28182 [==============================] - 1s - loss: 1.0087e-04     \n",
      "Epoch 8/50\n",
      "28182/28182 [==============================] - 5s - loss: 9.3674e-05     \n",
      "Epoch 9/50\n",
      "28182/28182 [==============================] - 2s - loss: 8.3456e-05     \n",
      "Epoch 10/50\n",
      "28182/28182 [==============================] - 1s - loss: 7.4513e-05     \n",
      "Epoch 11/50\n",
      "28182/28182 [==============================] - 3s - loss: 6.8755e-05     \n",
      "Epoch 12/50\n",
      "28182/28182 [==============================] - 5s - loss: 6.3337e-05     \n",
      "Epoch 13/50\n",
      "28182/28182 [==============================] - 1s - loss: 5.9286e-05     \n",
      "Epoch 14/50\n",
      "28182/28182 [==============================] - 1s - loss: 5.5270e-05     \n",
      "Epoch 15/50\n",
      "28182/28182 [==============================] - 3s - loss: 5.2103e-05     \n",
      "Epoch 16/50\n",
      "28182/28182 [==============================] - 4s - loss: 4.9442e-05     \n",
      "Epoch 17/50\n",
      "28182/28182 [==============================] - 1s - loss: 5.2650e-05     \n",
      "Epoch 18/50\n",
      "28182/28182 [==============================] - 1s - loss: 4.5955e-05     \n",
      "Epoch 19/50\n",
      "28182/28182 [==============================] - 2s - loss: 4.3671e-05     \n",
      "Epoch 20/50\n",
      "28182/28182 [==============================] - 5s - loss: 4.1599e-05     \n",
      "Epoch 21/50\n",
      "28182/28182 [==============================] - 1s - loss: 3.9174e-05     \n",
      "Epoch 22/50\n",
      "28182/28182 [==============================] - 1s - loss: 3.8118e-05     \n",
      "Epoch 23/50\n",
      "28182/28182 [==============================] - 5s - loss: 3.6059e-05     \n",
      "Epoch 24/50\n",
      "28182/28182 [==============================] - 2s - loss: 3.4302e-05     \n",
      "Epoch 25/50\n",
      "28182/28182 [==============================] - 1s - loss: 3.2413e-05     \n",
      "Epoch 26/50\n",
      "28182/28182 [==============================] - 1s - loss: 3.2448e-05     \n",
      "Epoch 27/50\n",
      "28182/28182 [==============================] - 4s - loss: 3.0799e-05     \n",
      "Epoch 28/50\n",
      "28182/28182 [==============================] - 4s - loss: 3.0387e-05     \n",
      "Epoch 29/50\n",
      "28182/28182 [==============================] - 1s - loss: 2.8837e-05     \n",
      "Epoch 30/50\n",
      "28182/28182 [==============================] - 4s - loss: 3.3836e-05     \n",
      "Epoch 31/50\n",
      "28182/28182 [==============================] - 4s - loss: 2.7945e-05     \n",
      "Epoch 32/50\n",
      "28182/28182 [==============================] - 1s - loss: 2.7868e-05     \n",
      "Epoch 33/50\n",
      "28182/28182 [==============================] - 1s - loss: 2.7244e-05     \n",
      "Epoch 34/50\n",
      "28182/28182 [==============================] - 1s - loss: 2.6155e-05     \n",
      "Epoch 35/50\n",
      "28182/28182 [==============================] - 3s - loss: 2.9818e-05     \n",
      "Epoch 36/50\n",
      "28182/28182 [==============================] - 5s - loss: 2.4753e-05     \n",
      "Epoch 37/50\n",
      "28182/28182 [==============================] - 1s - loss: 2.5131e-05     \n",
      "Epoch 38/50\n",
      "28182/28182 [==============================] - 3s - loss: 2.4145e-05     \n",
      "Epoch 39/50\n",
      "28182/28182 [==============================] - 5s - loss: 2.4539e-05     \n",
      "Epoch 40/50\n",
      "28182/28182 [==============================] - 1s - loss: 2.4008e-05     \n",
      "Epoch 41/50\n",
      "28182/28182 [==============================] - 2s - loss: 2.4392e-05     \n",
      "Epoch 42/50\n",
      "28182/28182 [==============================] - 5s - loss: 2.3891e-05     \n",
      "Epoch 43/50\n",
      "28182/28182 [==============================] - 1s - loss: 2.3037e-05     \n",
      "Epoch 44/50\n",
      "28182/28182 [==============================] - 2s - loss: 2.2987e-05     \n",
      "Epoch 45/50\n",
      "28182/28182 [==============================] - 5s - loss: 2.2853e-05     \n",
      "Epoch 46/50\n",
      "28182/28182 [==============================] - 2s - loss: 2.3054e-05     \n",
      "Epoch 47/50\n",
      "28182/28182 [==============================] - 1s - loss: 2.2233e-05     \n",
      "Epoch 48/50\n",
      "28182/28182 [==============================] - 5s - loss: 2.2949e-05     \n",
      "Epoch 49/50\n",
      "28182/28182 [==============================] - 2s - loss: 2.1645e-05     \n",
      "Epoch 50/50\n",
      "28182/28182 [==============================] - 2s - loss: 2.2444e-05     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1938d863f60>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train\n",
    "autoencoder.fit(x_train, x_train,\n",
    "                epochs=50,\n",
    "                batch_size=64,\n",
    "                shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((563640, 14), (290360, 14), numpy.ndarray, 563640, 290360)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_train = int(data.shape[0]*0.66) \n",
    "n_test = data.shape[0] - n_train\n",
    "\n",
    "x_train = data.loc[:n_train-1, ' AF3':' AF4'].values\n",
    "x_test = data.loc[n_train:, ' AF3':' AF4'].values\n",
    "\n",
    "x_train.shape, x_test.shape, type(x_train), n_train, n_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_dim = CHANNELS * TIMESTAMPS # 28 timepoints of 14 channels (about 0.23 seconds of data)\n",
    "datapoint_dim = [1, TIMESTAMPS, CHANNELS, 1]  # dimensionality of one datapoint\n",
    "\n",
    "# Convolutional autoencoder\n",
    "input_img = Input(shape=(TIMESTAMPS, CHANNELS, 1))  # The data will now be treated as an image \n",
    "\n",
    "x = Conv2D(16, (3, 3), activation='relu', padding='same')(input_img)\n",
    "encoded = MaxPooling2D((2, 2), padding='same')(x)\n",
    "\n",
    "# at this point the representation is (14, 7, 16) i.e. lot-dimensional\n",
    "\n",
    "x = Conv2D(16, (3, 3), activation='relu', padding=\"same\")(encoded)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "decoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n",
    "\n",
    "# The autoencoder\n",
    "autoencoder = Model(input_img, decoded)\n",
    "autoencoder.compile(optimizer='adam', loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this model maps an input to its encoded representation\n",
    "encoder = Model(input_img, encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create a placeholder for an encoded input\n",
    "encoded_input = Input(shape=(14,7,16))\n",
    "\n",
    "# retrieve the decoder layers of the autoencoder model\n",
    "decoder_layer = autoencoder.layers[-3](encoded_input)\n",
    "decoder_layer = autoencoder.layers[-2](decoder_layer)\n",
    "decoder_layer = autoencoder.layers[-1](decoder_layer)\n",
    "\n",
    "# create the decoder model\n",
    "decoder = Model(encoded_input, decoder_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((40260, 14, 14, 1), (20740, 14, 14, 1))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We will normalize all values between 0 and 1 and we will reshape to (None, 14, 14, 1), 14 timepoints with 14 signals\n",
    "shift = min(np.amin(x_train), np.amin(x_test))\n",
    "normalize = max(np.amax(x_train), np.amax(x_test)) - shift\n",
    "\n",
    "x_train = (x_train.astype('float32') - shift) / normalize\n",
    "x_test = (x_test.astype('float32') - shift) / normalize\n",
    "\n",
    "x_train = x_train.reshape(int(n_train/TIMESTAMPS), TIMESTAMPS, CHANNELS, 1)\n",
    "x_test = x_test.reshape(int(n_test/TIMESTAMPS), TIMESTAMPS, CHANNELS, 1)\n",
    "x_train.shape, x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "40260/40260 [==============================] - 5s - loss: 4.2666e-04     \n",
      "Epoch 2/50\n",
      "40260/40260 [==============================] - 4s - loss: 1.2728e-04     \n",
      "Epoch 3/50\n",
      "40260/40260 [==============================] - 4s - loss: 7.5080e-05     \n",
      "Epoch 4/50\n",
      "40260/40260 [==============================] - 4s - loss: 5.4494e-05     \n",
      "Epoch 5/50\n",
      "40260/40260 [==============================] - 4s - loss: 4.5717e-05     \n",
      "Epoch 6/50\n",
      "40260/40260 [==============================] - 4s - loss: 3.9599e-05     \n",
      "Epoch 7/50\n",
      "40260/40260 [==============================] - 4s - loss: 3.5018e-05     \n",
      "Epoch 8/50\n",
      "40260/40260 [==============================] - 4s - loss: 3.1095e-05     \n",
      "Epoch 9/50\n",
      "40260/40260 [==============================] - 4s - loss: 2.7111e-05     \n",
      "Epoch 10/50\n",
      "40260/40260 [==============================] - 4s - loss: 2.4937e-05     \n",
      "Epoch 11/50\n",
      "40260/40260 [==============================] - 4s - loss: 2.2641e-05     \n",
      "Epoch 12/50\n",
      "40260/40260 [==============================] - 4s - loss: 2.0995e-05     \n",
      "Epoch 13/50\n",
      "40260/40260 [==============================] - 4s - loss: 1.9406e-05     \n",
      "Epoch 14/50\n",
      "40260/40260 [==============================] - 4s - loss: 1.8869e-05     \n",
      "Epoch 15/50\n",
      "40260/40260 [==============================] - 4s - loss: 1.7754e-05     \n",
      "Epoch 16/50\n",
      "40260/40260 [==============================] - 4s - loss: 1.7043e-05     \n",
      "Epoch 17/50\n",
      "40260/40260 [==============================] - 4s - loss: 1.6717e-05     \n",
      "Epoch 18/50\n",
      "40260/40260 [==============================] - 4s - loss: 1.5825e-05     \n",
      "Epoch 19/50\n",
      "40260/40260 [==============================] - 4s - loss: 1.5565e-05     \n",
      "Epoch 20/50\n",
      "40260/40260 [==============================] - 4s - loss: 1.5056e-05     \n",
      "Epoch 21/50\n",
      "40260/40260 [==============================] - 4s - loss: 1.4646e-05     \n",
      "Epoch 22/50\n",
      "40260/40260 [==============================] - 4s - loss: 1.4274e-05     \n",
      "Epoch 23/50\n",
      "40260/40260 [==============================] - 4s - loss: 1.3798e-05     \n",
      "Epoch 24/50\n",
      "40260/40260 [==============================] - 4s - loss: 1.3818e-05     \n",
      "Epoch 25/50\n",
      "40260/40260 [==============================] - 4s - loss: 1.3206e-05     \n",
      "Epoch 26/50\n",
      "40260/40260 [==============================] - 4s - loss: 1.3036e-05     \n",
      "Epoch 27/50\n",
      "40260/40260 [==============================] - 4s - loss: 1.2766e-05     \n",
      "Epoch 28/50\n",
      "40260/40260 [==============================] - 4s - loss: 1.2492e-05     \n",
      "Epoch 29/50\n",
      "40260/40260 [==============================] - 4s - loss: 1.2229e-05     \n",
      "Epoch 30/50\n",
      "40260/40260 [==============================] - 4s - loss: 1.1926e-05     \n",
      "Epoch 31/50\n",
      "40260/40260 [==============================] - 4s - loss: 1.2011e-05     \n",
      "Epoch 32/50\n",
      "40260/40260 [==============================] - 4s - loss: 1.1470e-05     \n",
      "Epoch 33/50\n",
      "40260/40260 [==============================] - 4s - loss: 1.1081e-05     \n",
      "Epoch 34/50\n",
      "40260/40260 [==============================] - 4s - loss: 1.1152e-05     \n",
      "Epoch 35/50\n",
      "40260/40260 [==============================] - 4s - loss: 1.1063e-05     \n",
      "Epoch 36/50\n",
      "40260/40260 [==============================] - 4s - loss: 1.0570e-05     \n",
      "Epoch 37/50\n",
      "40260/40260 [==============================] - 4s - loss: 1.0618e-05     \n",
      "Epoch 38/50\n",
      "40260/40260 [==============================] - 4s - loss: 1.0495e-05     \n",
      "Epoch 39/50\n",
      "40260/40260 [==============================] - 4s - loss: 1.0297e-05     \n",
      "Epoch 40/50\n",
      "40260/40260 [==============================] - 4s - loss: 9.6347e-06     \n",
      "Epoch 41/50\n",
      "40260/40260 [==============================] - 4s - loss: 1.0004e-05     \n",
      "Epoch 42/50\n",
      "40260/40260 [==============================] - 4s - loss: 9.5901e-06     \n",
      "Epoch 43/50\n",
      "40260/40260 [==============================] - 4s - loss: 9.6703e-06     \n",
      "Epoch 44/50\n",
      "40260/40260 [==============================] - 4s - loss: 9.4882e-06     \n",
      "Epoch 45/50\n",
      "40260/40260 [==============================] - 4s - loss: 9.4052e-06     \n",
      "Epoch 46/50\n",
      "40260/40260 [==============================] - 4s - loss: 9.1712e-06     \n",
      "Epoch 47/50\n",
      "40260/40260 [==============================] - 4s - loss: 9.0866e-06     \n",
      "Epoch 48/50\n",
      "40260/40260 [==============================] - 4s - loss: 9.1751e-06     \n",
      "Epoch 49/50\n",
      "40260/40260 [==============================] - 4s - loss: 8.9835e-06     \n",
      "Epoch 50/50\n",
      "40260/40260 [==============================] - 4s - loss: 8.7070e-06     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x193b6d4e9b0>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train convolutional autoencoder\n",
    "autoencoder.fit(x_train, x_train,\n",
    "                epochs=50,\n",
    "                batch_size=64,\n",
    "                shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Denoising MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11520000,)\n",
      "0\n",
      "Size of Data in memory is approximately: 1406.25 MB.\n"
     ]
    }
   ],
   "source": [
    "with open(path + \"pure_signal_extraction_data_indices.mem\", \"rb\") as input_file:\n",
    "    indices = pickle.load(input_file)\n",
    "\n",
    "# we will get a sinusoid estimation of an eeg signal and a noisy variant to train the autoencoder on\n",
    "shuffled_indices = []\n",
    "pure_data = []\n",
    "noisy_data = []\n",
    "\n",
    "\n",
    "for j in range(50):\n",
    "    # load twelve hours of data\n",
    "    dat, shuffled_indices = random_signal_loader(indices, shuffled_indices, path + \"pure_signal_extraction_data.mem\")\n",
    "    pure_data.extend(dat[1])\n",
    "    noisy_data.extend(dat[0])\n",
    "        \n",
    "# reshape into correctly size numpy arrays\n",
    "pure_data = np.array(pure_data).T\n",
    "noisy_data = np.array(noisy_data).T\n",
    "        \n",
    "# some stats\n",
    "print(pure_data.shape)\n",
    "print(len(shuffled_indices))\n",
    "print(\"Size of Data in memory is approximately: \" +\n",
    "          str(NUMBER_OF_TRAINING_INSTANCES*2*sys.getsizeof(pure_data[0])*pure_data.shape[0]/1048576) + \" MB.\")\n",
    "\n",
    "init_val = 2500\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7603200,),\n",
       " (3916800,),\n",
       " (7603200,),\n",
       " (3916800,),\n",
       " numpy.ndarray,\n",
       " 7603200,\n",
       " 3916800,\n",
       " 0)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data dimensions\n",
    "input_dim = CHANNELS * TIMESTAMPS  # 14 timepoints of 14 channels (about 0.12 seconds of data)\n",
    "datapoint_dim = [1, CHANNELS * TIMESTAMPS]  # dimensionality of one datapoint, 128\n",
    "\n",
    "# stats\n",
    "n_train = int(pure_data.shape[0]*0.66) \n",
    "n_test = pure_data.shape[0] - n_train \n",
    "\n",
    "\n",
    "# non noisy\n",
    "x_train = set_creator(TIMESTAMPS, pure_data[:n_train])\n",
    "x_test = set_creator(TIMESTAMPS, pure_data[n_train:])\n",
    "\n",
    "# noisy\n",
    "x_train_noisy = set_creator(TIMESTAMPS, noisy_data[:n_train])\n",
    "x_test_noisy = set_creator(TIMESTAMPS, noisy_data[n_train:])\n",
    "\n",
    "# reworked stats\n",
    "n_train = x_train.shape[0]\n",
    "n_test = x_test.shape[0]\n",
    "\n",
    "x_train.shape, x_test.shape, x_train_noisy.shape, x_test_noisy.shape, type(x_train), n_train, n_test, n_train % TIMESTAMPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Deep autoencoder\n",
    "hidden_dim = 64  # For 280-dimensional input data (20*14) these layer sizes seem to suffice. Play with this if you change the input.\n",
    "encoding_dim = 32\n",
    "\n",
    "# this is our input placeholder\n",
    "input_eeg = Input(shape=(input_dim,))\n",
    "\n",
    "# \"encoded\" is the encoded representation of the input\n",
    "encoded = Dense(hidden_dim, activation=\"relu\")(input_eeg)\n",
    "encoded = Dense(hidden_dim, activation='relu')(encoded)\n",
    "encoded = Dense(encoding_dim, activation='relu')(encoded)\n",
    "\n",
    "# \"decoded\" is the lossy reconstruction of the input\n",
    "decoded = Dense(hidden_dim, activation='relu')(encoded)\n",
    "decoded = Dense(hidden_dim, activation='relu')(decoded)\n",
    "decoded = Dense(input_dim, activation='sigmoid')(decoded)\n",
    "\n",
    "# this model maps an input to its reconstruction\n",
    "autoencoder = Model(input_eeg, decoded)\n",
    "autoencoder.compile(optimizer='adam', loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# this model maps an input to its encoded representation\n",
    "encoder = Model(input_eeg, encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create a placeholder for an encoded input\n",
    "encoded_input = Input(shape=(encoding_dim,))\n",
    "\n",
    "# retrieve the decoder layers of the autoencoder model\n",
    "decoder_layer = autoencoder.layers[-3](encoded_input)\n",
    "decoder_layer = autoencoder.layers[-2](decoder_layer)\n",
    "decoder_layer = autoencoder.layers[-1](decoder_layer)\n",
    "\n",
    "# create the decoder model\n",
    "decoder = Model(encoded_input, decoder_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((118800, 64), (61200, 64), (118800, 64), (61200, 64))"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We will normalize all values between 0 and 1 and we will reshape to (None, input_dim)\n",
    "shift = min(np.amin(x_train_noisy), np.amin(x_test_noisy), np.amin(x_train), np.amin(x_test))\n",
    "normalize = max(np.amax(x_train_noisy), np.amax(x_test_noisy), np.amax(x_train), np.amax(x_test)) - shift\n",
    "\n",
    "x_train = (x_train.astype('float32') - shift) / normalize\n",
    "x_train_noisy = (x_train_noisy.astype('float32') - shift) / normalize\n",
    "x_test = (x_test.astype('float32') - shift) / normalize\n",
    "x_test_noisy = (x_test_noisy.astype('float32') - shift) / normalize\n",
    "\n",
    "x_train = x_train.reshape(int(n_train/input_dim), input_dim)\n",
    "x_test = x_test.reshape(int(n_test/input_dim), input_dim)\n",
    "x_train_noisy = x_train_noisy.reshape(int(n_train/input_dim), input_dim)\n",
    "x_test_noisy = x_test_noisy.reshape(int(n_test/input_dim), input_dim)\n",
    "x_train.shape, x_test.shape, x_train_noisy.shape, x_test_noisy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "118800/118800 [==============================] - 2s - loss: 0.0026     \n",
      "Epoch 2/100\n",
      "118800/118800 [==============================] - 2s - loss: 7.0875e-05     \n",
      "Epoch 3/100\n",
      "118800/118800 [==============================] - 2s - loss: 5.1110e-05     \n",
      "Epoch 4/100\n",
      "118800/118800 [==============================] - 2s - loss: 3.8329e-05     \n",
      "Epoch 5/100\n",
      "118800/118800 [==============================] - 2s - loss: 3.3839e-05     \n",
      "Epoch 6/100\n",
      "118800/118800 [==============================] - 2s - loss: 3.0374e-05     \n",
      "Epoch 7/100\n",
      "118800/118800 [==============================] - 2s - loss: 2.6475e-05     \n",
      "Epoch 8/100\n",
      "118800/118800 [==============================] - 2s - loss: 3.6652e-05     \n",
      "Epoch 9/100\n",
      "118800/118800 [==============================] - 2s - loss: 2.3476e-05     \n",
      "Epoch 10/100\n",
      "118800/118800 [==============================] - 2s - loss: 2.5177e-05     \n",
      "Epoch 11/100\n",
      "118800/118800 [==============================] - 2s - loss: 2.4231e-05     \n",
      "Epoch 12/100\n",
      "118800/118800 [==============================] - 2s - loss: 2.3245e-05     \n",
      "Epoch 13/100\n",
      "118800/118800 [==============================] - 2s - loss: 2.1421e-05     \n",
      "Epoch 14/100\n",
      "118800/118800 [==============================] - 2s - loss: 2.3123e-05     \n",
      "Epoch 15/100\n",
      "118800/118800 [==============================] - 2s - loss: 2.2461e-05     \n",
      "Epoch 16/100\n",
      "118800/118800 [==============================] - 2s - loss: 2.1392e-05     \n",
      "Epoch 17/100\n",
      "118800/118800 [==============================] - 2s - loss: 2.2000e-05     \n",
      "Epoch 18/100\n",
      "118800/118800 [==============================] - 2s - loss: 2.1291e-05     \n",
      "Epoch 19/100\n",
      "118800/118800 [==============================] - 2s - loss: 2.0946e-05     \n",
      "Epoch 20/100\n",
      "118800/118800 [==============================] - 2s - loss: 2.1083e-05     \n",
      "Epoch 21/100\n",
      "118800/118800 [==============================] - 2s - loss: 2.0480e-05     \n",
      "Epoch 22/100\n",
      "118800/118800 [==============================] - 2s - loss: 2.0558e-05     \n",
      "Epoch 23/100\n",
      "118800/118800 [==============================] - 2s - loss: 2.0118e-05     \n",
      "Epoch 24/100\n",
      "118800/118800 [==============================] - 2s - loss: 2.0801e-05     \n",
      "Epoch 25/100\n",
      "118800/118800 [==============================] - 2s - loss: 1.9895e-05     \n",
      "Epoch 26/100\n",
      "118800/118800 [==============================] - 2s - loss: 1.9843e-05     \n",
      "Epoch 27/100\n",
      "118800/118800 [==============================] - 2s - loss: 1.9815e-05     \n",
      "Epoch 28/100\n",
      "118800/118800 [==============================] - 2s - loss: 1.9544e-05     \n",
      "Epoch 29/100\n",
      "118800/118800 [==============================] - 2s - loss: 2.0047e-05     \n",
      "Epoch 30/100\n",
      "118800/118800 [==============================] - 2s - loss: 1.9669e-05     \n",
      "Epoch 31/100\n",
      "118800/118800 [==============================] - 2s - loss: 1.9535e-05     \n",
      "Epoch 32/100\n",
      "118800/118800 [==============================] - 2s - loss: 1.9822e-05     \n",
      "Epoch 33/100\n",
      "118800/118800 [==============================] - 2s - loss: 1.8939e-05     \n",
      "Epoch 34/100\n",
      "118800/118800 [==============================] - 2s - loss: 1.9335e-05     \n",
      "Epoch 35/100\n",
      "118800/118800 [==============================] - 2s - loss: 1.8668e-05     \n",
      "Epoch 36/100\n",
      "118800/118800 [==============================] - 2s - loss: 1.9381e-05     \n",
      "Epoch 37/100\n",
      "118800/118800 [==============================] - 2s - loss: 1.8387e-05     \n",
      "Epoch 38/100\n",
      "118800/118800 [==============================] - 2s - loss: 1.8832e-05     \n",
      "Epoch 39/100\n",
      "118800/118800 [==============================] - 2s - loss: 1.9519e-05     \n",
      "Epoch 40/100\n",
      "118800/118800 [==============================] - 2s - loss: 1.8802e-05     \n",
      "Epoch 41/100\n",
      "118800/118800 [==============================] - 2s - loss: 1.9391e-05     \n",
      "Epoch 42/100\n",
      "118800/118800 [==============================] - 2s - loss: 1.8894e-05     \n",
      "Epoch 43/100\n",
      "118800/118800 [==============================] - 2s - loss: 1.8259e-05     \n",
      "Epoch 44/100\n",
      "118800/118800 [==============================] - 2s - loss: 1.8561e-05     \n",
      "Epoch 45/100\n",
      "118800/118800 [==============================] - 2s - loss: 1.9306e-05     \n",
      "Epoch 46/100\n",
      "118800/118800 [==============================] - 2s - loss: 1.8482e-05     \n",
      "Epoch 47/100\n",
      "118800/118800 [==============================] - 2s - loss: 1.8614e-05     \n",
      "Epoch 48/100\n",
      "118800/118800 [==============================] - 2s - loss: 1.8710e-05     \n",
      "Epoch 49/100\n",
      "118800/118800 [==============================] - 2s - loss: 1.8286e-05     \n",
      "Epoch 50/100\n",
      "118800/118800 [==============================] - 2s - loss: 1.8540e-05     \n",
      "Epoch 51/100\n",
      "118800/118800 [==============================] - 2s - loss: 1.8778e-05     \n",
      "Epoch 52/100\n",
      "118800/118800 [==============================] - 2s - loss: 1.8796e-05     \n",
      "Epoch 53/100\n",
      "118800/118800 [==============================] - 2s - loss: 1.7993e-05     \n",
      "Epoch 54/100\n",
      "118800/118800 [==============================] - 2s - loss: 1.8247e-05     \n",
      "Epoch 55/100\n",
      "118800/118800 [==============================] - 2s - loss: 1.8360e-05     \n",
      "Epoch 56/100\n",
      "118800/118800 [==============================] - 2s - loss: 1.8415e-05     \n",
      "Epoch 57/100\n",
      "118800/118800 [==============================] - 2s - loss: 1.8148e-05     \n",
      "Epoch 58/100\n",
      "118800/118800 [==============================] - 2s - loss: 1.8644e-05     \n",
      "Epoch 59/100\n",
      "118800/118800 [==============================] - 2s - loss: 1.8002e-05     \n",
      "Epoch 60/100\n",
      "118800/118800 [==============================] - 2s - loss: 1.8114e-05     \n",
      "Epoch 61/100\n",
      "118800/118800 [==============================] - 2s - loss: 1.8185e-05     \n",
      "Epoch 62/100\n",
      "118800/118800 [==============================] - 2s - loss: 1.8353e-05     \n",
      "Epoch 63/100\n",
      "118800/118800 [==============================] - 2s - loss: 1.9225e-05     \n",
      "Epoch 64/100\n",
      "118800/118800 [==============================] - 2s - loss: 1.7437e-05     \n",
      "Epoch 65/100\n",
      "118800/118800 [==============================] - 2s - loss: 1.7959e-05     \n",
      "Epoch 66/100\n",
      "118800/118800 [==============================] - 2s - loss: 1.8625e-05     \n",
      "Epoch 67/100\n",
      "118800/118800 [==============================] - 2s - loss: 1.7935e-05     \n",
      "Epoch 68/100\n",
      "118800/118800 [==============================] - 2s - loss: 1.8602e-05     \n",
      "Epoch 69/100\n",
      "118800/118800 [==============================] - 2s - loss: 1.7614e-05     \n",
      "Epoch 70/100\n",
      "118800/118800 [==============================] - 2s - loss: 1.7400e-05     \n",
      "Epoch 71/100\n",
      "118800/118800 [==============================] - 2s - loss: 1.8249e-05     \n",
      "Epoch 72/100\n",
      "118800/118800 [==============================] - 2s - loss: 1.8121e-05     \n",
      "Epoch 73/100\n",
      "118800/118800 [==============================] - 2s - loss: 1.7866e-05     \n",
      "Epoch 74/100\n",
      "118800/118800 [==============================] - 2s - loss: 1.7762e-05     \n",
      "Epoch 75/100\n",
      "118800/118800 [==============================] - 2s - loss: 1.7691e-05     \n",
      "Epoch 76/100\n",
      "118800/118800 [==============================] - 2s - loss: 1.8524e-05     \n",
      "Epoch 77/100\n",
      "118800/118800 [==============================] - 2s - loss: 1.7231e-05     \n",
      "Epoch 78/100\n",
      "118800/118800 [==============================] - 2s - loss: 1.8885e-05     \n",
      "Epoch 79/100\n",
      "118800/118800 [==============================] - 2s - loss: 1.7202e-05     \n",
      "Epoch 80/100\n",
      "118800/118800 [==============================] - 2s - loss: 1.8022e-05     \n",
      "Epoch 81/100\n",
      "118800/118800 [==============================] - 2s - loss: 1.8464e-05     \n",
      "Epoch 82/100\n",
      "118800/118800 [==============================] - 2s - loss: 1.7161e-05     \n",
      "Epoch 83/100\n",
      "118800/118800 [==============================] - 2s - loss: 1.7519e-05     \n",
      "Epoch 84/100\n",
      "118800/118800 [==============================] - 2s - loss: 1.7743e-05     \n",
      "Epoch 85/100\n",
      "118800/118800 [==============================] - 2s - loss: 1.7876e-05     \n",
      "Epoch 86/100\n",
      "118800/118800 [==============================] - 2s - loss: 1.8182e-05     \n",
      "Epoch 87/100\n",
      "118800/118800 [==============================] - 2s - loss: 1.7795e-05     \n",
      "Epoch 88/100\n",
      "118800/118800 [==============================] - 2s - loss: 1.7932e-05     \n",
      "Epoch 89/100\n",
      "118800/118800 [==============================] - 2s - loss: 1.7582e-05     \n",
      "Epoch 90/100\n",
      "118800/118800 [==============================] - 2s - loss: 1.7501e-05     \n",
      "Epoch 91/100\n",
      "118800/118800 [==============================] - 2s - loss: 1.7365e-05     \n",
      "Epoch 92/100\n",
      "118800/118800 [==============================] - 2s - loss: 1.7982e-05     \n",
      "Epoch 93/100\n",
      "118800/118800 [==============================] - 2s - loss: 1.7604e-05     \n",
      "Epoch 94/100\n",
      "118800/118800 [==============================] - 2s - loss: 1.7976e-05     \n",
      "Epoch 95/100\n",
      "118800/118800 [==============================] - 2s - loss: 1.7202e-05     \n",
      "Epoch 96/100\n",
      "118800/118800 [==============================] - 2s - loss: 1.7625e-05     \n",
      "Epoch 97/100\n",
      "118800/118800 [==============================] - 2s - loss: 1.7676e-05     \n",
      "Epoch 98/100\n",
      "118800/118800 [==============================] - 2s - loss: 1.7936e-05     \n",
      "Epoch 99/100\n",
      "118800/118800 [==============================] - 2s - loss: 1.7147e-05     \n",
      "Epoch 100/100\n",
      "118800/118800 [==============================] - 2s - loss: 1.7815e-05     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1dd71b27ac8>"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train convolutional autoencoder\n",
    "autoencoder.fit(x_train_noisy, x_train,\n",
    "                epochs=100,\n",
    "                batch_size=256,\n",
    "                shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Denoising autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1843200, 14)\n",
      "451\n",
      "Size of Data in memory is approximately: 225.0 MB.\n"
     ]
    }
   ],
   "source": [
    "with open(path + \"pure_signal_extraction_data_indices.mem\", \"rb\") as input_file:\n",
    "    indices = pickle.load(input_file)\n",
    "\n",
    "# we will get a sinusoid estimation of an eeg signal and a noisy variant to train the autoencoder on\n",
    "shuffled_indices = []\n",
    "pure_data = []\n",
    "noisy_data = []\n",
    "\n",
    "for i in range(14):\n",
    "    pure_data.append([])\n",
    "    noisy_data.append([])\n",
    "    for j in range(8):\n",
    "        # load four hours of data\n",
    "        dat, shuffled_indices = random_signal_loader(indices, shuffled_indices, path + \"pure_signal_extraction_data.mem\")\n",
    "        pure_data[i].extend(dat[1])\n",
    "        noisy_data[i].extend(dat[0])\n",
    "        \n",
    "# reshape into correctly size numpy arrays\n",
    "pure_data = np.array(pure_data).T\n",
    "noisy_data = np.array(noisy_data).T\n",
    "        \n",
    "# some stats\n",
    "print(pure_data.shape)\n",
    "print(len(shuffled_indices))\n",
    "print(\"Size of Data in memory is approximately: \" +\n",
    "          str(NUMBER_OF_TRAINING_INSTANCES*2*sys.getsizeof(pure_data[0][0])*pure_data.shape[0]/1048576) + \" MB.\")\n",
    "\n",
    "init_val = 2500\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1216512, 14),\n",
       " (626688, 14),\n",
       " (1216512, 14),\n",
       " (626688, 14),\n",
       " numpy.ndarray,\n",
       " 1216512,\n",
       " 626688,\n",
       " 0)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data dimensions\n",
    "input_dim = CHANNELS * TIMESTAMPS  # 14 timepoints of 14 channels (about 0.12 seconds of data)\n",
    "datapoint_dim = [1,TIMESTAMPS, CHANNELS,1]  # dimensionality of one datapoint\n",
    "\n",
    "# stats\n",
    "n_train = int(pure_data.shape[0]*0.66) \n",
    "n_test = pure_data.shape[0] - n_train \n",
    "\n",
    "\n",
    "# non noisy\n",
    "x_train = set_creator(TIMESTAMPS, pure_data[:n_train, :])\n",
    "x_test = set_creator(TIMESTAMPS, pure_data[n_train:, :])\n",
    "\n",
    "# noisy\n",
    "x_train_noisy = set_creator(TIMESTAMPS, noisy_data[:n_train, :])\n",
    "x_test_noisy = set_creator(TIMESTAMPS, noisy_data[n_train:, :])\n",
    "\n",
    "# reworked stats\n",
    "n_train = x_train.shape[0]\n",
    "n_test = x_test.shape[0]\n",
    "\n",
    "x_train.shape, x_test.shape, x_train_noisy.shape, x_test_noisy.shape, type(x_train), n_train, n_test, n_train % TIMESTAMPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Convolutional autoencoder\n",
    "input_img = Input(shape=(TIMESTAMPS, CHANNELS, 1))  # The data will now be treated as a 14 by 14 image \n",
    "\n",
    "x = Conv2D(32, (3, 3), activation='relu', padding='same')(input_img)\n",
    "x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "x = MaxPooling2D((2, 1), padding='same')(x)\n",
    "x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "encoded = MaxPooling2D((2, 1), padding='same')(x)\n",
    "\n",
    "# at this point the representation is (16, 4, 32) i.e. lot-dimensional\n",
    "\n",
    "x = Conv2D(32, (3, 3), activation='relu', padding=\"same\")(encoded)\n",
    "x = UpSampling2D((2, 1))(x)\n",
    "x = Conv2D(32, (3, 3), activation='relu', padding=\"same\")(x)\n",
    "x = UpSampling2D((2, 1))(x)\n",
    "x = Conv2D(32, (3, 3), activation='relu', padding=\"same\")(x)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "x = Conv2D(32, (1, 2), activation='relu')(x)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "decoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n",
    "\n",
    "\n",
    "# The autoencoder\n",
    "autoencoder = Model(input_img, decoded)\n",
    "autoencoder.compile(optimizer='adam', loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this model maps an input to its encoded representation\n",
    "encoder = Model(input_img, encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a placeholder for an encoded input\n",
    "encoded_input = Input(shape=(16,4,32))\n",
    "\n",
    "# retrieve the decoder layers of the autoencoder model\n",
    "decoder_layer = autoencoder.layers[-9](encoded_input)\n",
    "decoder_layer = autoencoder.layers[-8](decoder_layer)\n",
    "decoder_layer = autoencoder.layers[-7](decoder_layer)\n",
    "decoder_layer = autoencoder.layers[-6](decoder_layer)\n",
    "decoder_layer = autoencoder.layers[-5](decoder_layer)\n",
    "decoder_layer = autoencoder.layers[-4](decoder_layer)\n",
    "decoder_layer = autoencoder.layers[-3](decoder_layer)\n",
    "decoder_layer = autoencoder.layers[-2](decoder_layer)\n",
    "decoder_layer = autoencoder.layers[-1](decoder_layer)\n",
    "\n",
    "# create the decoder model\n",
    "decoder = Model(encoded_input, decoder_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((118800, 64, 1, 1), (61200, 64, 1, 1), (118800, 64, 1, 1), (61200, 64, 1, 1))"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We will normalize all values between 0 and 1 and we will reshape to (None, 14, 14, 1), 14 timepoints with 14 signals\n",
    "shift = min(np.amin(x_train_noisy), np.amin(x_test_noisy), np.amin(x_train), np.amin(x_test))\n",
    "normalize = max(np.amax(x_train_noisy), np.amax(x_test_noisy), np.amax(x_train), np.amax(x_test)) - shift\n",
    "\n",
    "x_train = (x_train.astype('float32') - shift) / normalize\n",
    "x_train_noisy = (x_train_noisy.astype('float32') - shift) / normalize\n",
    "x_test = (x_test.astype('float32') - shift) / normalize\n",
    "x_test_noisy = (x_test_noisy.astype('float32') - shift) / normalize\n",
    "\n",
    "x_train = x_train.reshape(int(n_train/TIMESTAMPS), TIMESTAMPS, CHANNELS, 1)\n",
    "x_test = x_test.reshape(int(n_test/TIMESTAMPS), TIMESTAMPS, CHANNELS, 1)\n",
    "x_train_noisy = x_train_noisy.reshape(int(n_train/TIMESTAMPS), TIMESTAMPS, CHANNELS, 1)\n",
    "x_test_noisy = x_test_noisy.reshape(int(n_test/TIMESTAMPS), TIMESTAMPS, CHANNELS, 1)\n",
    "x_train.shape, x_test.shape, x_train_noisy.shape, x_test_noisy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "4752/4752 [==============================] - 6s - loss: 0.0015     \n",
      "Epoch 2/50\n",
      "4752/4752 [==============================] - 6s - loss: 0.0015     \n",
      "Epoch 3/50\n",
      "4752/4752 [==============================] - 6s - loss: 0.0015     \n",
      "Epoch 4/50\n",
      "4752/4752 [==============================] - 6s - loss: 0.0015     \n",
      "Epoch 5/50\n",
      "4752/4752 [==============================] - 6s - loss: 0.0015     \n",
      "Epoch 6/50\n",
      "4752/4752 [==============================] - 6s - loss: 0.0015     \n",
      "Epoch 7/50\n",
      "4752/4752 [==============================] - 6s - loss: 0.0015     \n",
      "Epoch 8/50\n",
      "4752/4752 [==============================] - 6s - loss: 0.0015     \n",
      "Epoch 9/50\n",
      "4752/4752 [==============================] - 6s - loss: 0.0015     \n",
      "Epoch 10/50\n",
      "4752/4752 [==============================] - 6s - loss: 0.0015     \n",
      "Epoch 11/50\n",
      "4752/4752 [==============================] - 6s - loss: 0.0015     \n",
      "Epoch 12/50\n",
      "4752/4752 [==============================] - 6s - loss: 0.0014     \n",
      "Epoch 13/50\n",
      "4752/4752 [==============================] - 6s - loss: 0.0014     \n",
      "Epoch 14/50\n",
      "4752/4752 [==============================] - 6s - loss: 0.0014     \n",
      "Epoch 15/50\n",
      "4752/4752 [==============================] - 6s - loss: 0.0014     \n",
      "Epoch 16/50\n",
      "4752/4752 [==============================] - 6s - loss: 0.0014     \n",
      "Epoch 17/50\n",
      "4752/4752 [==============================] - 6s - loss: 0.0014     \n",
      "Epoch 18/50\n",
      "4752/4752 [==============================] - 6s - loss: 0.0014     \n",
      "Epoch 19/50\n",
      "4752/4752 [==============================] - 6s - loss: 0.0014     \n",
      "Epoch 20/50\n",
      "4752/4752 [==============================] - 6s - loss: 0.0014     \n",
      "Epoch 21/50\n",
      "4752/4752 [==============================] - 6s - loss: 0.0014     \n",
      "Epoch 22/50\n",
      "4752/4752 [==============================] - 6s - loss: 0.0014     \n",
      "Epoch 23/50\n",
      "4752/4752 [==============================] - 6s - loss: 0.0014     \n",
      "Epoch 24/50\n",
      "4752/4752 [==============================] - 6s - loss: 0.0014     \n",
      "Epoch 25/50\n",
      "4752/4752 [==============================] - 6s - loss: 0.0014     \n",
      "Epoch 26/50\n",
      "4752/4752 [==============================] - 6s - loss: 0.0014     \n",
      "Epoch 27/50\n",
      "4752/4752 [==============================] - 6s - loss: 0.0014     \n",
      "Epoch 28/50\n",
      "4752/4752 [==============================] - 6s - loss: 0.0014     \n",
      "Epoch 29/50\n",
      "4752/4752 [==============================] - 6s - loss: 0.0014     \n",
      "Epoch 30/50\n",
      " 256/4752 [>.............................] - ETA: 6s - loss: 0.0014"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-125-0c442f355f9f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m                 \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m                 \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m                 shuffle=True)\n\u001b[0m",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[0;32m   1505\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1506\u001b[0m                               \u001b[0mcallback_metrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1507\u001b[1;33m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1508\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[1;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\u001b[0m\n\u001b[0;32m   1154\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'size'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1155\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1156\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1157\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1158\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2267\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[0;32m   2268\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2269\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2270\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2271\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    776\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 778\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    779\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    980\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 982\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    983\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1030\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1032\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1033\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1037\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1040\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[0;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1021\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1022\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1023\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train convolutional autoencoder\n",
    "autoencoder.fit(x_train_noisy, x_train,\n",
    "                epochs=50,\n",
    "                batch_size=64,\n",
    "                shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(61200, 64)\n",
      "(61200, 64)\n",
      "(61200, 32)\n"
     ]
    }
   ],
   "source": [
    "# encode and decode \n",
    "encoded_eegs = encoder.predict(x_test_noisy)\n",
    "decoded_eegs = decoder.predict(encoded_eegs)\n",
    "\n",
    "# these should have the same shape\n",
    "print(x_test_noisy.shape)\n",
    "print(decoded_eegs.shape)\n",
    "\n",
    "# this should have the correct encoded dimensions\n",
    "print(encoded_eegs.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import correlation, euclidean, mahalanobis, minkowski,cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corr: 0.934709749555\n",
      "euc: 0.07881008088588715\n",
      "cos: 0.000234588267086\n",
      "cheb: 0.0285109\n"
     ]
    }
   ],
   "source": [
    "# Get a sample from the test set\n",
    "sample = x_test_noisy[10].reshape(datapoint_dim)\n",
    "encoded_x = encoder.predict(sample)\n",
    "decoded_x = decoder.predict(encoded_x)\n",
    "\n",
    "# Apply different distance measures in order to evaluate the decoding (extend this for grid search) ()\n",
    "test_x = sample.reshape(input_dim)\n",
    "test_y = decoded_x.reshape(input_dim)\n",
    "print(\"corr: \" + str(correlation(test_x,test_y)))\n",
    "print(\"euc: \" + str(euclidean(test_x,test_y)))\n",
    "print(\"cos: \" + str(cosine(test_x,test_y))) # probably the best measure due to the high dimensionality of the data\n",
    "print(\"cheb: \" + str(minkowski(test_x,test_y,np.inf))) # chebyshev distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEACAYAAACznAEdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1dd397666a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Draw a plot to compare a slice of the test signal and its decoded reconstruction\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "x_test_noisy = x_test_noisy.reshape(n_test, CHANNELS)\n",
    "x_test = x_test.reshape(n_test, CHANNELS)\n",
    "decoded_eegs = decoded_eegs.reshape((n_test, CHANNELS))\n",
    "plt.plot(noisy_data[999990:1000200], label=\"noisy\")\n",
    "plt.plot(pure_data[999990:1000200], label=\"pure\")\n",
    "# plt.plot(decoded_eegs[:10000], label = \"reconstructed\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
