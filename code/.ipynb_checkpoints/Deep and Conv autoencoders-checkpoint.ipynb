{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.layers import Input, Dense, Conv2D, MaxPooling2D, UpSampling2D\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "import pickle\n",
    "import random as rand\n",
    "import time\n",
    "import sys\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# globals\n",
    "NUMBER_OF_TRAINING_INSTANCES = 2\n",
    "CHANNELS = 14\n",
    "TIMESTAMPS = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"denoising_autoencoder_unsupervised.json\", \"r\") as json_file:\n",
    "    model_final = model_from_json(json_file.read())\n",
    "model_final.load_weights(\"denoising_autoencoder_unsupervised_data.hdf5\")\n",
    "model_final.compile(loss='mse', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# random signal loader, for loading toy data\n",
    "def random_signal_loader(static_indices, shuffled_non_static_indices, file_path):\n",
    "    \"\"\"random signal loader, for loading fake EEG data\"\"\"\n",
    "    training_instance = False\n",
    "    \n",
    "    if(len(shuffled_non_static_indices) == 0):\n",
    "        shuffled_non_static_indices = list(static_indices)\n",
    "        rand.shuffle(shuffled_non_static_indices)\n",
    "    with open(file_path, \"rb\") as input_file:\n",
    "        input_file.seek(shuffled_non_static_indices.pop(0))\n",
    "        training_instance = pickle.load(input_file)\n",
    "        \n",
    "    return training_instance, shuffled_non_static_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize_signals(pure_signal, no_missing_noisy_signal):\n",
    "    normalized_pure_signal = pure_signal - np.mean(pure_signal, axis=0)\n",
    "    normalized_estimated_signal = no_missing_noisy_signal - np.mean(no_missing_noisy_signal, axis=0)\n",
    "    min_ = min(np.amin(normalized_pure_signal), np.amin(normalized_estimated_signal))\n",
    "    normalized_pure_signal -= min_\n",
    "    normalized_estimated_signal -= min_\n",
    "    max_ = max(np.amax(normalized_pure_signal), np.amax(normalized_estimated_signal))\n",
    "    normalized_pure_signal /= max_\n",
    "    normalized_estimated_signal /= max_\n",
    "\n",
    "    return normalized_pure_signal, normalized_estimated_signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test and training set size selector, for cropping the data\n",
    "def set_creator(input_size, data_set):\n",
    "    size = data_set.shape[0]\n",
    "    reduce = size % input_size\n",
    "    if reduce != 0:\n",
    "        return data_set[:-reduce,:]\n",
    "    else:\n",
    "        return data_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path = os.getcwd()\n",
    "data = pd.read_csv(path + '\\\\preprocessed_old.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data2 = pd.read_csv(path + '\\\\preprocessed_new.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1314000, 37)\n",
      "(1456761, 52)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COUNTER</th>\n",
       "      <th>INTERPOLATED</th>\n",
       "      <th>AF3</th>\n",
       "      <th>F7</th>\n",
       "      <th>F3</th>\n",
       "      <th>FC5</th>\n",
       "      <th>T7</th>\n",
       "      <th>P7</th>\n",
       "      <th>O1</th>\n",
       "      <th>O2</th>\n",
       "      <th>...</th>\n",
       "      <th>GYROY_md</th>\n",
       "      <th>GYROZ</th>\n",
       "      <th>ACCX</th>\n",
       "      <th>ACCY</th>\n",
       "      <th>ACCZ</th>\n",
       "      <th>MAGX</th>\n",
       "      <th>MAGY</th>\n",
       "      <th>MAGZ</th>\n",
       "      <th>TIME_STAMP_s_md</th>\n",
       "      <th>mask</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>102.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>78.286767</td>\n",
       "      <td>-4.17838</td>\n",
       "      <td>-4.441258</td>\n",
       "      <td>-9.226732</td>\n",
       "      <td>-11.700883</td>\n",
       "      <td>-6.188615</td>\n",
       "      <td>-6.441889</td>\n",
       "      <td>-10.214907</td>\n",
       "      <td>...</td>\n",
       "      <td>8186.153809</td>\n",
       "      <td>8211.794922</td>\n",
       "      <td>4528.205078</td>\n",
       "      <td>8244.102539</td>\n",
       "      <td>5930.256348</td>\n",
       "      <td>8662.050781</td>\n",
       "      <td>8061.025391</td>\n",
       "      <td>8162.051270</td>\n",
       "      <td>346.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>103.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>78.286767</td>\n",
       "      <td>-4.17838</td>\n",
       "      <td>-4.441258</td>\n",
       "      <td>-9.226732</td>\n",
       "      <td>-11.700883</td>\n",
       "      <td>-6.188615</td>\n",
       "      <td>-6.441889</td>\n",
       "      <td>-10.214907</td>\n",
       "      <td>...</td>\n",
       "      <td>8186.153809</td>\n",
       "      <td>8211.794922</td>\n",
       "      <td>4528.205078</td>\n",
       "      <td>8244.102539</td>\n",
       "      <td>5930.256348</td>\n",
       "      <td>8662.050781</td>\n",
       "      <td>8061.025391</td>\n",
       "      <td>8162.051270</td>\n",
       "      <td>346.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>104.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>78.286767</td>\n",
       "      <td>-4.17838</td>\n",
       "      <td>-4.441258</td>\n",
       "      <td>-9.226732</td>\n",
       "      <td>-11.700883</td>\n",
       "      <td>-6.188615</td>\n",
       "      <td>-6.441889</td>\n",
       "      <td>-10.214907</td>\n",
       "      <td>...</td>\n",
       "      <td>8190.769043</td>\n",
       "      <td>8209.230469</td>\n",
       "      <td>4541.025391</td>\n",
       "      <td>8238.974609</td>\n",
       "      <td>5949.230469</td>\n",
       "      <td>8655.897461</td>\n",
       "      <td>8058.974121</td>\n",
       "      <td>8161.025391</td>\n",
       "      <td>346.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>105.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>78.286767</td>\n",
       "      <td>-4.17838</td>\n",
       "      <td>-4.441258</td>\n",
       "      <td>-9.226732</td>\n",
       "      <td>-11.700883</td>\n",
       "      <td>-6.188615</td>\n",
       "      <td>-6.441889</td>\n",
       "      <td>-10.214907</td>\n",
       "      <td>...</td>\n",
       "      <td>8190.769043</td>\n",
       "      <td>8209.230469</td>\n",
       "      <td>4541.025391</td>\n",
       "      <td>8238.974609</td>\n",
       "      <td>5949.230469</td>\n",
       "      <td>8655.897461</td>\n",
       "      <td>8058.974121</td>\n",
       "      <td>8161.025391</td>\n",
       "      <td>346.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>106.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>78.286767</td>\n",
       "      <td>-4.17838</td>\n",
       "      <td>-4.441258</td>\n",
       "      <td>-9.226732</td>\n",
       "      <td>-11.700883</td>\n",
       "      <td>-6.188615</td>\n",
       "      <td>-6.441889</td>\n",
       "      <td>-10.214907</td>\n",
       "      <td>...</td>\n",
       "      <td>8187.179199</td>\n",
       "      <td>8204.102539</td>\n",
       "      <td>4512.820312</td>\n",
       "      <td>8190.769043</td>\n",
       "      <td>5997.948730</td>\n",
       "      <td>8656.922852</td>\n",
       "      <td>8058.974121</td>\n",
       "      <td>8158.974121</td>\n",
       "      <td>346.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>107.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>78.286767</td>\n",
       "      <td>-4.17838</td>\n",
       "      <td>-4.441258</td>\n",
       "      <td>-9.226732</td>\n",
       "      <td>-11.700883</td>\n",
       "      <td>-6.188615</td>\n",
       "      <td>-6.441889</td>\n",
       "      <td>-10.214907</td>\n",
       "      <td>...</td>\n",
       "      <td>8187.179199</td>\n",
       "      <td>8204.102539</td>\n",
       "      <td>4512.820312</td>\n",
       "      <td>8190.769043</td>\n",
       "      <td>5997.948730</td>\n",
       "      <td>8656.922852</td>\n",
       "      <td>8058.974121</td>\n",
       "      <td>8158.974121</td>\n",
       "      <td>346.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>108.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>78.286767</td>\n",
       "      <td>-4.17838</td>\n",
       "      <td>-4.441258</td>\n",
       "      <td>-9.226732</td>\n",
       "      <td>-11.700883</td>\n",
       "      <td>-6.188615</td>\n",
       "      <td>-6.441889</td>\n",
       "      <td>-10.214907</td>\n",
       "      <td>...</td>\n",
       "      <td>8181.025391</td>\n",
       "      <td>8190.769043</td>\n",
       "      <td>4509.230469</td>\n",
       "      <td>8308.205078</td>\n",
       "      <td>5977.948730</td>\n",
       "      <td>8656.922852</td>\n",
       "      <td>8056.922852</td>\n",
       "      <td>8157.948730</td>\n",
       "      <td>347.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>109.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>78.286767</td>\n",
       "      <td>-4.17838</td>\n",
       "      <td>-4.441258</td>\n",
       "      <td>-9.226732</td>\n",
       "      <td>-11.700883</td>\n",
       "      <td>-6.188615</td>\n",
       "      <td>-6.441889</td>\n",
       "      <td>-10.214907</td>\n",
       "      <td>...</td>\n",
       "      <td>8181.025391</td>\n",
       "      <td>8190.769043</td>\n",
       "      <td>4509.230469</td>\n",
       "      <td>8308.205078</td>\n",
       "      <td>5977.948730</td>\n",
       "      <td>8656.922852</td>\n",
       "      <td>8056.922852</td>\n",
       "      <td>8157.948730</td>\n",
       "      <td>347.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>110.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>78.286767</td>\n",
       "      <td>-4.17838</td>\n",
       "      <td>-4.441258</td>\n",
       "      <td>-9.226732</td>\n",
       "      <td>-11.700883</td>\n",
       "      <td>-6.188615</td>\n",
       "      <td>-6.441889</td>\n",
       "      <td>-10.214907</td>\n",
       "      <td>...</td>\n",
       "      <td>8186.153809</td>\n",
       "      <td>8190.769043</td>\n",
       "      <td>4536.922852</td>\n",
       "      <td>8225.127930</td>\n",
       "      <td>5944.102539</td>\n",
       "      <td>8660.000000</td>\n",
       "      <td>8060.000000</td>\n",
       "      <td>8162.051270</td>\n",
       "      <td>347.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>111.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>78.286767</td>\n",
       "      <td>-4.17838</td>\n",
       "      <td>-4.441258</td>\n",
       "      <td>-9.226732</td>\n",
       "      <td>-11.700883</td>\n",
       "      <td>-6.188615</td>\n",
       "      <td>-6.441889</td>\n",
       "      <td>-10.214907</td>\n",
       "      <td>...</td>\n",
       "      <td>8186.153809</td>\n",
       "      <td>8190.769043</td>\n",
       "      <td>4536.922852</td>\n",
       "      <td>8225.127930</td>\n",
       "      <td>5944.102539</td>\n",
       "      <td>8660.000000</td>\n",
       "      <td>8060.000000</td>\n",
       "      <td>8162.051270</td>\n",
       "      <td>347.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 52 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   COUNTER  INTERPOLATED        AF3       F7        F3       FC5         T7  \\\n",
       "0    102.0           0.0  78.286767 -4.17838 -4.441258 -9.226732 -11.700883   \n",
       "1    103.0           0.0  78.286767 -4.17838 -4.441258 -9.226732 -11.700883   \n",
       "2    104.0           0.0  78.286767 -4.17838 -4.441258 -9.226732 -11.700883   \n",
       "3    105.0           0.0  78.286767 -4.17838 -4.441258 -9.226732 -11.700883   \n",
       "4    106.0           0.0  78.286767 -4.17838 -4.441258 -9.226732 -11.700883   \n",
       "5    107.0           0.0  78.286767 -4.17838 -4.441258 -9.226732 -11.700883   \n",
       "6    108.0           0.0  78.286767 -4.17838 -4.441258 -9.226732 -11.700883   \n",
       "7    109.0           0.0  78.286767 -4.17838 -4.441258 -9.226732 -11.700883   \n",
       "8    110.0           0.0  78.286767 -4.17838 -4.441258 -9.226732 -11.700883   \n",
       "9    111.0           0.0  78.286767 -4.17838 -4.441258 -9.226732 -11.700883   \n",
       "\n",
       "         P7        O1         O2  ...      GYROY_md        GYROZ         ACCX  \\\n",
       "0 -6.188615 -6.441889 -10.214907  ...   8186.153809  8211.794922  4528.205078   \n",
       "1 -6.188615 -6.441889 -10.214907  ...   8186.153809  8211.794922  4528.205078   \n",
       "2 -6.188615 -6.441889 -10.214907  ...   8190.769043  8209.230469  4541.025391   \n",
       "3 -6.188615 -6.441889 -10.214907  ...   8190.769043  8209.230469  4541.025391   \n",
       "4 -6.188615 -6.441889 -10.214907  ...   8187.179199  8204.102539  4512.820312   \n",
       "5 -6.188615 -6.441889 -10.214907  ...   8187.179199  8204.102539  4512.820312   \n",
       "6 -6.188615 -6.441889 -10.214907  ...   8181.025391  8190.769043  4509.230469   \n",
       "7 -6.188615 -6.441889 -10.214907  ...   8181.025391  8190.769043  4509.230469   \n",
       "8 -6.188615 -6.441889 -10.214907  ...   8186.153809  8190.769043  4536.922852   \n",
       "9 -6.188615 -6.441889 -10.214907  ...   8186.153809  8190.769043  4536.922852   \n",
       "\n",
       "          ACCY         ACCZ         MAGX         MAGY         MAGZ  \\\n",
       "0  8244.102539  5930.256348  8662.050781  8061.025391  8162.051270   \n",
       "1  8244.102539  5930.256348  8662.050781  8061.025391  8162.051270   \n",
       "2  8238.974609  5949.230469  8655.897461  8058.974121  8161.025391   \n",
       "3  8238.974609  5949.230469  8655.897461  8058.974121  8161.025391   \n",
       "4  8190.769043  5997.948730  8656.922852  8058.974121  8158.974121   \n",
       "5  8190.769043  5997.948730  8656.922852  8058.974121  8158.974121   \n",
       "6  8308.205078  5977.948730  8656.922852  8056.922852  8157.948730   \n",
       "7  8308.205078  5977.948730  8656.922852  8056.922852  8157.948730   \n",
       "8  8225.127930  5944.102539  8660.000000  8060.000000  8162.051270   \n",
       "9  8225.127930  5944.102539  8660.000000  8060.000000  8162.051270   \n",
       "\n",
       "   TIME_STAMP_s_md  mask  \n",
       "0            346.0   0.0  \n",
       "1            346.0   0.0  \n",
       "2            346.0   0.0  \n",
       "3            346.0   0.0  \n",
       "4            346.0   0.0  \n",
       "5            346.0   0.0  \n",
       "6            347.0   0.0  \n",
       "7            347.0   0.0  \n",
       "8            347.0   0.0  \n",
       "9            347.0   0.0  \n",
       "\n",
       "[10 rows x 52 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check whether everything looks okay, then proceed\n",
    "print(data.shape)\n",
    "print(data2.shape)\n",
    "data.head(10)\n",
    "data2.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tlpel\\Documents\\Studie\\AI1\\pAI\\code\\unsupervised data\\old\\\n",
      "(78000, 36)\n",
      "(80000, 36)\n",
      "(79000, 36)\n",
      "(80000, 36)\n",
      "(77000, 36)\n",
      "(191000, 36)\n",
      "(269000, 36)\n",
      "(460000, 36)\n"
     ]
    }
   ],
   "source": [
    "# Load the unsupervised data from folder, make sure the folder name is unsupervised data and in your path\n",
    "dataset = []\n",
    "path = os.getcwd() + '\\\\unsupervised data\\\\old\\\\'\n",
    "print(path)\n",
    "for file in os.listdir(path):\n",
    "    if file.endswith(\".CSV\"):\n",
    "        data3 = pd.read_csv(path + file)\n",
    "        print(data3.shape)\n",
    "        dataset.append(data3)\n",
    "    '''if file.endswith(\".edf\"):\n",
    "        print(path+file)\n",
    "        reader = pyedflib.EdfReader(path+file)\n",
    "        print(reader)'''\n",
    "\n",
    "# Get one long data file\n",
    "data3 = pd.concat(dataset, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path = os.getcwd() + '\\\\unsupervised data\\\\'\n",
    "dataset = []\n",
    "names = []\n",
    "i = -1\n",
    "db1 = 0\n",
    "db2 = 0\n",
    "for file in os.listdir(path):\n",
    "    if file.endswith(\".md.csv\"):\n",
    "        i += 1\n",
    "        db2 = pd.read_csv(path + file, skiprows=[0])\n",
    "        header = {}\n",
    "        f = open(path + file, 'r')\n",
    "        for h in f.readline().split(','):\n",
    "            hh = h.split(':')\n",
    "            header[hh[0].replace(' ', '')] = hh[1].split()\n",
    "        f.close()\n",
    "        db2.columns = header['labels']\n",
    "        db2.TIME_STAMP_ms += db2.TIME_STAMP_s * 1000\n",
    "    elif file.endswith(\".csv\"):\n",
    "        i += 1\n",
    "        names.append(file)\n",
    "        db1 = pd.read_csv(path + file, skiprows=[0])\n",
    "        header = {}\n",
    "        f = open(path + file, 'r')\n",
    "        for h in f.readline().split(','):\n",
    "            hh = h.split(':')\n",
    "            header[hh[0].replace(' ', '')] = hh[1].split()\n",
    "        f.close()\n",
    "        db1.columns = header['labels']\n",
    "        db1.TIME_STAMP_ms += db1.TIME_STAMP_s * 1000\n",
    "\n",
    "    if i % 2 == 1 and not file.endswith('old'):\n",
    "        data = pd.merge(db1, db2, on='TIME_STAMP_ms', how='left', suffixes=['_eeg', '_md'])\n",
    "        header['labels'] = list(data.columns)\n",
    "        dataset.append(data.ffill().bfill())\n",
    "        \n",
    "# Get one long data file\n",
    "data4 = pd.concat(dataset, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1314000, 37)\n",
      "(1456761, 52)\n",
      "(1314000, 36)\n",
      "(1456761, 51)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COUNTER</th>\n",
       "      <th>INTERPOLATED</th>\n",
       "      <th>AF3</th>\n",
       "      <th>F7</th>\n",
       "      <th>F3</th>\n",
       "      <th>FC5</th>\n",
       "      <th>T7</th>\n",
       "      <th>P7</th>\n",
       "      <th>O1</th>\n",
       "      <th>O2</th>\n",
       "      <th>...</th>\n",
       "      <th>CQ_FC6</th>\n",
       "      <th>CQ_F4</th>\n",
       "      <th>CQ_F8</th>\n",
       "      <th>CQ_AF4</th>\n",
       "      <th>CQ_CMS</th>\n",
       "      <th>CQ_DRL</th>\n",
       "      <th>GYROX</th>\n",
       "      <th>GYROY</th>\n",
       "      <th>MARKER</th>\n",
       "      <th>mask</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>53.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-14.082761</td>\n",
       "      <td>-5.233143</td>\n",
       "      <td>-9.073006</td>\n",
       "      <td>-4.894746</td>\n",
       "      <td>-43.470163</td>\n",
       "      <td>-6.084496</td>\n",
       "      <td>-4.089003</td>\n",
       "      <td>-15.152149</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1672.0</td>\n",
       "      <td>1679.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>54.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-14.082761</td>\n",
       "      <td>-5.233143</td>\n",
       "      <td>-9.073006</td>\n",
       "      <td>-4.894746</td>\n",
       "      <td>-43.470163</td>\n",
       "      <td>-6.084496</td>\n",
       "      <td>-4.089003</td>\n",
       "      <td>-15.152149</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1672.0</td>\n",
       "      <td>1679.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>55.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-14.082761</td>\n",
       "      <td>-5.233143</td>\n",
       "      <td>-9.073006</td>\n",
       "      <td>-4.894746</td>\n",
       "      <td>-43.470163</td>\n",
       "      <td>-6.084496</td>\n",
       "      <td>-4.089003</td>\n",
       "      <td>-15.152149</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1670.0</td>\n",
       "      <td>1680.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-14.082761</td>\n",
       "      <td>-5.233143</td>\n",
       "      <td>-9.073006</td>\n",
       "      <td>-4.894746</td>\n",
       "      <td>-43.470163</td>\n",
       "      <td>-6.084496</td>\n",
       "      <td>-4.089003</td>\n",
       "      <td>-15.152149</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1670.0</td>\n",
       "      <td>1682.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>57.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-14.082761</td>\n",
       "      <td>-5.233143</td>\n",
       "      <td>-9.073006</td>\n",
       "      <td>-4.894746</td>\n",
       "      <td>-43.470163</td>\n",
       "      <td>-6.084496</td>\n",
       "      <td>-4.089003</td>\n",
       "      <td>-15.152149</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1669.0</td>\n",
       "      <td>1683.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>58.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-14.082761</td>\n",
       "      <td>-5.233143</td>\n",
       "      <td>-9.073006</td>\n",
       "      <td>-4.894746</td>\n",
       "      <td>-43.470163</td>\n",
       "      <td>-6.084496</td>\n",
       "      <td>-4.089003</td>\n",
       "      <td>-15.152149</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1669.0</td>\n",
       "      <td>1686.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>59.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-14.082761</td>\n",
       "      <td>-5.233143</td>\n",
       "      <td>-9.073006</td>\n",
       "      <td>-4.894746</td>\n",
       "      <td>-43.470163</td>\n",
       "      <td>-6.084496</td>\n",
       "      <td>-4.089003</td>\n",
       "      <td>-15.152149</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1670.0</td>\n",
       "      <td>1685.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>60.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-14.082761</td>\n",
       "      <td>-5.233143</td>\n",
       "      <td>-9.073006</td>\n",
       "      <td>-4.894746</td>\n",
       "      <td>-43.470163</td>\n",
       "      <td>-6.084496</td>\n",
       "      <td>-4.089003</td>\n",
       "      <td>-15.152149</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1668.0</td>\n",
       "      <td>1687.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>61.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-14.082761</td>\n",
       "      <td>-5.233143</td>\n",
       "      <td>-9.073006</td>\n",
       "      <td>-4.894746</td>\n",
       "      <td>-43.470163</td>\n",
       "      <td>-6.084496</td>\n",
       "      <td>-4.089003</td>\n",
       "      <td>-15.152149</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1670.0</td>\n",
       "      <td>1686.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>62.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-14.082761</td>\n",
       "      <td>-5.233143</td>\n",
       "      <td>-9.073006</td>\n",
       "      <td>-4.894746</td>\n",
       "      <td>-43.470163</td>\n",
       "      <td>-6.084496</td>\n",
       "      <td>-4.089003</td>\n",
       "      <td>-15.152149</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1669.0</td>\n",
       "      <td>1686.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   COUNTER   INTERPOLATED        AF3        F7        F3       FC5         T7  \\\n",
       "0     53.0            0.0 -14.082761 -5.233143 -9.073006 -4.894746 -43.470163   \n",
       "1     54.0            0.0 -14.082761 -5.233143 -9.073006 -4.894746 -43.470163   \n",
       "2     55.0            0.0 -14.082761 -5.233143 -9.073006 -4.894746 -43.470163   \n",
       "3     56.0            0.0 -14.082761 -5.233143 -9.073006 -4.894746 -43.470163   \n",
       "4     57.0            0.0 -14.082761 -5.233143 -9.073006 -4.894746 -43.470163   \n",
       "5     58.0            0.0 -14.082761 -5.233143 -9.073006 -4.894746 -43.470163   \n",
       "6     59.0            0.0 -14.082761 -5.233143 -9.073006 -4.894746 -43.470163   \n",
       "7     60.0            0.0 -14.082761 -5.233143 -9.073006 -4.894746 -43.470163   \n",
       "8     61.0            0.0 -14.082761 -5.233143 -9.073006 -4.894746 -43.470163   \n",
       "9     62.0            0.0 -14.082761 -5.233143 -9.073006 -4.894746 -43.470163   \n",
       "\n",
       "         P7        O1         O2  ...    CQ_FC6   CQ_F4   CQ_F8   CQ_AF4  \\\n",
       "0 -6.084496 -4.089003 -15.152149  ...       4.0     4.0     4.0      4.0   \n",
       "1 -6.084496 -4.089003 -15.152149  ...       4.0     4.0     4.0      4.0   \n",
       "2 -6.084496 -4.089003 -15.152149  ...       4.0     4.0     4.0      4.0   \n",
       "3 -6.084496 -4.089003 -15.152149  ...       4.0     4.0     4.0      4.0   \n",
       "4 -6.084496 -4.089003 -15.152149  ...       4.0     4.0     4.0      4.0   \n",
       "5 -6.084496 -4.089003 -15.152149  ...       4.0     4.0     4.0      4.0   \n",
       "6 -6.084496 -4.089003 -15.152149  ...       4.0     4.0     4.0      4.0   \n",
       "7 -6.084496 -4.089003 -15.152149  ...       4.0     4.0     4.0      4.0   \n",
       "8 -6.084496 -4.089003 -15.152149  ...       4.0     4.0     4.0      4.0   \n",
       "9 -6.084496 -4.089003 -15.152149  ...       4.0     4.0     4.0      4.0   \n",
       "\n",
       "    CQ_CMS   CQ_DRL   GYROX   GYROY   MARKER  mask  \n",
       "0      4.0      4.0  1672.0  1679.0      0.0   0.0  \n",
       "1      4.0      4.0  1672.0  1679.0      0.0   0.0  \n",
       "2      4.0      4.0  1670.0  1680.0      0.0   0.0  \n",
       "3      4.0      4.0  1670.0  1682.0      0.0   0.0  \n",
       "4      4.0      4.0  1669.0  1683.0      0.0   0.0  \n",
       "5      4.0      4.0  1669.0  1686.0      0.0   0.0  \n",
       "6      4.0      4.0  1670.0  1685.0      0.0   0.0  \n",
       "7      4.0      4.0  1668.0  1687.0      0.0   0.0  \n",
       "8      4.0      4.0  1670.0  1686.0      0.0   0.0  \n",
       "9      4.0      4.0  1669.0  1686.0      0.0   0.0  \n",
       "\n",
       "[10 rows x 37 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check whether everything looks okay, then proceed\n",
    "print(data.shape)\n",
    "print(data2.shape)\n",
    "print(data3.shape)\n",
    "print(data4.shape)\n",
    "data.head(10)\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Deep Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Do a 66/34 split of training and test. Play a bit with this to get nice sized training and test sets.\n",
    "n_train = int(data.shape[0]*0.66) \n",
    "n_test = data.shape[0] - n_train\n",
    "\n",
    "x_train = data.loc[:n_train-1, ' AF3':' AF4'].values\n",
    "x_test = data.loc[n_train:, ' AF3':' AF4'].values\n",
    "\n",
    "x_train.shape, x_test.shape, type(x_train), n_train, n_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Deep autoencoder\n",
    "datapoint_dim = [1, CHANNELS * TIMESTAMPS]  # dimensionality of one datapoint\n",
    "input_dim = CHANNELS * TIMESTAMPS # 28 timepoints of 14 channels (about 0.23 seconds of data)\n",
    "hidden_dim = 64  # For 280-dimensional input data (20*14) these layer sizes seem to suffice. Play with this if you change the input.\n",
    "encoding_dim = 32\n",
    "\n",
    "# this is our input placeholder\n",
    "input_eeg = Input(shape=(input_dim,))\n",
    "\n",
    "# \"encoded\" is the encoded representation of the input\n",
    "encoded = Dense(hidden_dim, activation=\"relu\")(input_eeg)\n",
    "encoded = Dense(encoding_dim, activation='relu')(encoded)\n",
    "\n",
    "# \"decoded\" is the lossy reconstruction of the input\n",
    "decoded = Dense(hidden_dim, activation='relu')(encoded)\n",
    "decoded = Dense(input_dim, activation='sigmoid')(decoded)\n",
    "\n",
    "# this model maps an input to its reconstruction\n",
    "autoencoder = Model(input_eeg, decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# this model maps an input to its encoded representation\n",
    "encoder = Model(input_eeg, encoded)\n",
    "\n",
    "# this model maps an encoded representation to a reconstruction\n",
    "# decoder = Model(encoded, decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create a placeholder for an encoded input\n",
    "encoded_input = Input(shape=(encoding_dim,))\n",
    "\n",
    "# retrieve the decoder layers of the autoencoder model\n",
    "decoder_layer = autoencoder.layers[-2](encoded_input)\n",
    "decoder_layer = autoencoder.layers[-1](decoder_layer)\n",
    "\n",
    "# create the decoder model\n",
    "decoder = Model(encoded_input, decoder_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "autoencoder.compile(optimizer='adam', loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((28182, 280), (14518, 280))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We will normalize all values between 0 and 1 and we will flatten the 28x28 images into vectors of size 784.\n",
    "\n",
    "normalize = max(np.amax(x_train), np.amax(x_test))\n",
    "\n",
    "x_train = x_train.astype('float32') / normalize\n",
    "x_test = x_test.astype('float32') / normalize\n",
    "x_train = x_train.reshape(int(n_train/20.0), 14*20)\n",
    "x_test = x_test.reshape(int(n_test/20.0), 14*20)\n",
    "x_train.shape, x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "28182/28182 [==============================] - 8s - loss: 2.9815e-04     \n",
      "Epoch 2/50\n",
      "28182/28182 [==============================] - 2s - loss: 1.6537e-04     \n",
      "Epoch 3/50\n",
      "28182/28182 [==============================] - 5s - loss: 1.4896e-04     \n",
      "Epoch 4/50\n",
      "28182/28182 [==============================] - 1s - loss: 1.3800e-04     \n",
      "Epoch 5/50\n",
      "28182/28182 [==============================] - 2s - loss: 1.2190e-04     \n",
      "Epoch 6/50\n",
      "28182/28182 [==============================] - 5s - loss: 1.1251e-04     \n",
      "Epoch 7/50\n",
      "28182/28182 [==============================] - 1s - loss: 1.0087e-04     \n",
      "Epoch 8/50\n",
      "28182/28182 [==============================] - 5s - loss: 9.3674e-05     \n",
      "Epoch 9/50\n",
      "28182/28182 [==============================] - 2s - loss: 8.3456e-05     \n",
      "Epoch 10/50\n",
      "28182/28182 [==============================] - 1s - loss: 7.4513e-05     \n",
      "Epoch 11/50\n",
      "28182/28182 [==============================] - 3s - loss: 6.8755e-05     \n",
      "Epoch 12/50\n",
      "28182/28182 [==============================] - 5s - loss: 6.3337e-05     \n",
      "Epoch 13/50\n",
      "28182/28182 [==============================] - 1s - loss: 5.9286e-05     \n",
      "Epoch 14/50\n",
      "28182/28182 [==============================] - 1s - loss: 5.5270e-05     \n",
      "Epoch 15/50\n",
      "28182/28182 [==============================] - 3s - loss: 5.2103e-05     \n",
      "Epoch 16/50\n",
      "28182/28182 [==============================] - 4s - loss: 4.9442e-05     \n",
      "Epoch 17/50\n",
      "28182/28182 [==============================] - 1s - loss: 5.2650e-05     \n",
      "Epoch 18/50\n",
      "28182/28182 [==============================] - 1s - loss: 4.5955e-05     \n",
      "Epoch 19/50\n",
      "28182/28182 [==============================] - 2s - loss: 4.3671e-05     \n",
      "Epoch 20/50\n",
      "28182/28182 [==============================] - 5s - loss: 4.1599e-05     \n",
      "Epoch 21/50\n",
      "28182/28182 [==============================] - 1s - loss: 3.9174e-05     \n",
      "Epoch 22/50\n",
      "28182/28182 [==============================] - 1s - loss: 3.8118e-05     \n",
      "Epoch 23/50\n",
      "28182/28182 [==============================] - 5s - loss: 3.6059e-05     \n",
      "Epoch 24/50\n",
      "28182/28182 [==============================] - 2s - loss: 3.4302e-05     \n",
      "Epoch 25/50\n",
      "28182/28182 [==============================] - 1s - loss: 3.2413e-05     \n",
      "Epoch 26/50\n",
      "28182/28182 [==============================] - 1s - loss: 3.2448e-05     \n",
      "Epoch 27/50\n",
      "28182/28182 [==============================] - 4s - loss: 3.0799e-05     \n",
      "Epoch 28/50\n",
      "28182/28182 [==============================] - 4s - loss: 3.0387e-05     \n",
      "Epoch 29/50\n",
      "28182/28182 [==============================] - 1s - loss: 2.8837e-05     \n",
      "Epoch 30/50\n",
      "28182/28182 [==============================] - 4s - loss: 3.3836e-05     \n",
      "Epoch 31/50\n",
      "28182/28182 [==============================] - 4s - loss: 2.7945e-05     \n",
      "Epoch 32/50\n",
      "28182/28182 [==============================] - 1s - loss: 2.7868e-05     \n",
      "Epoch 33/50\n",
      "28182/28182 [==============================] - 1s - loss: 2.7244e-05     \n",
      "Epoch 34/50\n",
      "28182/28182 [==============================] - 1s - loss: 2.6155e-05     \n",
      "Epoch 35/50\n",
      "28182/28182 [==============================] - 3s - loss: 2.9818e-05     \n",
      "Epoch 36/50\n",
      "28182/28182 [==============================] - 5s - loss: 2.4753e-05     \n",
      "Epoch 37/50\n",
      "28182/28182 [==============================] - 1s - loss: 2.5131e-05     \n",
      "Epoch 38/50\n",
      "28182/28182 [==============================] - 3s - loss: 2.4145e-05     \n",
      "Epoch 39/50\n",
      "28182/28182 [==============================] - 5s - loss: 2.4539e-05     \n",
      "Epoch 40/50\n",
      "28182/28182 [==============================] - 1s - loss: 2.4008e-05     \n",
      "Epoch 41/50\n",
      "28182/28182 [==============================] - 2s - loss: 2.4392e-05     \n",
      "Epoch 42/50\n",
      "28182/28182 [==============================] - 5s - loss: 2.3891e-05     \n",
      "Epoch 43/50\n",
      "28182/28182 [==============================] - 1s - loss: 2.3037e-05     \n",
      "Epoch 44/50\n",
      "28182/28182 [==============================] - 2s - loss: 2.2987e-05     \n",
      "Epoch 45/50\n",
      "28182/28182 [==============================] - 5s - loss: 2.2853e-05     \n",
      "Epoch 46/50\n",
      "28182/28182 [==============================] - 2s - loss: 2.3054e-05     \n",
      "Epoch 47/50\n",
      "28182/28182 [==============================] - 1s - loss: 2.2233e-05     \n",
      "Epoch 48/50\n",
      "28182/28182 [==============================] - 5s - loss: 2.2949e-05     \n",
      "Epoch 49/50\n",
      "28182/28182 [==============================] - 2s - loss: 2.1645e-05     \n",
      "Epoch 50/50\n",
      "28182/28182 [==============================] - 2s - loss: 2.2444e-05     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1938d863f60>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train\n",
    "autoencoder.fit(x_train, x_train,\n",
    "                epochs=50,\n",
    "                batch_size=64,\n",
    "                shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mask1 = data.loc[:, \"mask\"].values\n",
    "mask3 = data2.loc[:, \"mask\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data =  data.loc[:, ' AF3':' AF4'].values\n",
    "data = data[mask1 > 0]\n",
    "data2 =  data2.loc[:, 'AF3':'AF4'].values\n",
    "data2 = data2[mask3 > 0]\n",
    "data3 =  data3.loc[:, ' AF3':' AF4'].values\n",
    "data3 = data3[mask1 > 0]\n",
    "data4 =  data4.loc[:, 'AF3':'AF4'].values\n",
    "data4 = data4[mask3 > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1041534, 14)\n",
      "(1056344, 14)\n",
      "(1041534, 14)\n",
      "(1056344, 14)\n"
     ]
    }
   ],
   "source": [
    "print(data.shape)\n",
    "print(data2.shape)\n",
    "print(data3.shape)\n",
    "print(data4.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clean_data = np.concatenate((data, data2))\n",
    "noisy_data = np.concatenate((data3, data4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2097878, 14)\n",
      "(2097878, 14)\n"
     ]
    }
   ],
   "source": [
    "print(clean_data.shape)\n",
    "print(noisy_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1888064, 14), (209728, 14), numpy.ndarray, 1888064, 209728)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_train = int(clean_data.shape[0]*0.9) \n",
    "n_test = clean_data.shape[0] - n_train\n",
    "\n",
    "x_train = set_creator(TIMESTAMPS, clean_data[:n_train,:])\n",
    "x_test = set_creator(TIMESTAMPS, clean_data[n_train:,:])\n",
    "\n",
    "x_train_noisy = set_creator(TIMESTAMPS, noisy_data[:n_train,:])\n",
    "x_test_noisy = set_creator(TIMESTAMPS, noisy_data[n_train:,:])\n",
    "\n",
    "n_train = x_train.shape[0]\n",
    "n_test = x_test.shape[0]\n",
    "\n",
    "x_train.shape, x_test.shape, type(x_train), n_train, n_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_dim = CHANNELS * TIMESTAMPS # 64 timepoints of 14 channels (about 0.23 seconds of data)\n",
    "datapoint_dim = [1, TIMESTAMPS, CHANNELS, 1]  # dimensionality of one datapoint\n",
    "\n",
    "# Convolutional autoencoder\n",
    "input_img = Input(shape=(TIMESTAMPS, CHANNELS, 1))  # The data will now be treated as an image \n",
    "\n",
    "x = Conv2D(32, (3, 3), activation='relu', padding='same')(input_img)\n",
    "x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = Conv2D(16, (3, 3), activation='relu', padding='same')(x)\n",
    "encoded = MaxPooling2D((2, 1), padding='same')(x)\n",
    "\n",
    "\n",
    "# at this point the representation is (16, 7, 16) i.e. lot-dimensional\n",
    "\n",
    "\n",
    "x = Conv2D(16, (3, 3), activation='relu', padding=\"same\")(encoded)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "x = Conv2D(32, (3, 3), activation='relu', padding=\"same\")(x)\n",
    "x = UpSampling2D((2, 1))(x)\n",
    "decoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n",
    "\n",
    "# The autoencoder\n",
    "autoencoder = Model(input_img, decoded)\n",
    "autoencoder.compile(optimizer='adam', loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this model maps an input to its encoded representation\n",
    "encoder = Model(input_img, encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create a placeholder for an encoded input\n",
    "encoded_input = Input(shape=(16,7,16))\n",
    "\n",
    "# retrieve the decoder layers of the autoencoder model\n",
    "decoder_layer = autoencoder.layers[-5](encoded_input)\n",
    "decoder_layer = autoencoder.layers[-4](decoder_layer)\n",
    "decoder_layer = autoencoder.layers[-3](decoder_layer)\n",
    "decoder_layer = autoencoder.layers[-2](decoder_layer)\n",
    "decoder_layer = autoencoder.layers[-1](decoder_layer)\n",
    "\n",
    "# create the decoder model\n",
    "decoder = Model(encoded_input, decoder_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_final_json = autoencoder.to_json()\n",
    "\n",
    "with open(\"denoising_autoencoder_unsupervised.json\", \"w\") as json_file:\n",
    "    json_file.write(model_final_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((29501, 64, 14, 1), (3277, 64, 14, 1), (29501, 64, 14, 1), (3277, 64, 14, 1))"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We will normalize all values between 0 and 1 and we will reshape to (None, 14, 14, 1), 14 timepoints with 14 signals\n",
    "shift = min(np.amin(x_train), np.amin(x_test))\n",
    "normalize = max(np.amax(x_train), np.amax(x_test)) - shift\n",
    "\n",
    "x_train = (x_train.astype('float32') - shift) / normalize\n",
    "x_test = (x_test.astype('float32') - shift) / normalize\n",
    "\n",
    "x_train = x_train.reshape(int(n_train/TIMESTAMPS), TIMESTAMPS, CHANNELS, 1)\n",
    "x_test = x_test.reshape(int(n_test/TIMESTAMPS), TIMESTAMPS, CHANNELS, 1)\n",
    "x_train.shape, x_test.shape\n",
    "\n",
    "shift = min(np.amin(x_train_noisy), np.amin(x_test_noisy))\n",
    "normalize = max(np.amax(x_train_noisy), np.amax(x_test_noisy)) - shift\n",
    "\n",
    "x_train_noisy = (x_train_noisy.astype('float32') - shift) / normalize\n",
    "x_test_noisy = (x_test_noisy.astype('float32') - shift) / normalize\n",
    "\n",
    "x_train_noisy = x_train_noisy.reshape(int(n_train/TIMESTAMPS), TIMESTAMPS, CHANNELS, 1)\n",
    "x_test_noisy = x_test_noisy.reshape(int(n_test/TIMESTAMPS), TIMESTAMPS, CHANNELS, 1)\n",
    "x_train.shape, x_test.shape, x_train_noisy.shape, x_test_noisy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((29501, 64, 14, 1), (3277, 64, 14, 1), (29501, 64, 14, 1), (3277, 64, 14, 1))"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train, x_train_noisy = normalize_signals(x_train, x_train_noisy)\n",
    "x_test, x_test_noisy = normalize_signals(x_test, x_test_noisy)\n",
    "\n",
    "x_train = x_train.reshape(int(n_train/TIMESTAMPS), TIMESTAMPS, CHANNELS, 1)\n",
    "x_test = x_test.reshape(int(n_test/TIMESTAMPS), TIMESTAMPS, CHANNELS, 1)\n",
    "x_train.shape, x_test.shape\n",
    "\n",
    "x_train_noisy = x_train_noisy.reshape(int(n_train/TIMESTAMPS), TIMESTAMPS, CHANNELS, 1)\n",
    "x_test_noisy = x_test_noisy.reshape(int(n_test/TIMESTAMPS), TIMESTAMPS, CHANNELS, 1)\n",
    "x_train.shape, x_test.shape, x_train_noisy.shape, x_test_noisy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 26550 samples, validate on 2951 samples\n",
      "Epoch 1/50\n",
      "26550/26550 [==============================] - 12s - loss: 2.0915e-04 - val_loss: 2.4455e-04\n",
      "Epoch 2/50\n",
      "26550/26550 [==============================] - 11s - loss: 2.0880e-04 - val_loss: 2.4396e-04\n",
      "Epoch 3/50\n",
      "26550/26550 [==============================] - 11s - loss: 2.0441e-04 - val_loss: 2.3860e-04\n",
      "Epoch 4/50\n",
      "26550/26550 [==============================] - 11s - loss: 2.0050e-04 - val_loss: 2.3655e-04\n",
      "Epoch 5/50\n",
      "26550/26550 [==============================] - 11s - loss: 1.9867e-04 - val_loss: 2.3562e-04\n",
      "Epoch 6/50\n",
      "26550/26550 [==============================] - 11s - loss: 1.9634e-04 - val_loss: 2.2846e-04\n",
      "Epoch 7/50\n",
      "26550/26550 [==============================] - 11s - loss: 1.9359e-04 - val_loss: 2.1967e-04\n",
      "Epoch 8/50\n",
      "26550/26550 [==============================] - 11s - loss: 1.8986e-04 - val_loss: 2.2251e-04\n",
      "Epoch 9/50\n",
      "26550/26550 [==============================] - 11s - loss: 1.8542e-04 - val_loss: 2.0227e-04\n",
      "Epoch 10/50\n",
      "26550/26550 [==============================] - 11s - loss: 1.8099e-04 - val_loss: 1.9771e-04\n",
      "Epoch 11/50\n",
      "26550/26550 [==============================] - 11s - loss: 1.7864e-04 - val_loss: 2.0212e-04\n",
      "Epoch 12/50\n",
      "26550/26550 [==============================] - 11s - loss: 1.7619e-04 - val_loss: 1.9474e-04\n",
      "Epoch 13/50\n",
      "26550/26550 [==============================] - 11s - loss: 1.7537e-04 - val_loss: 1.9553e-04\n",
      "Epoch 14/50\n",
      "26550/26550 [==============================] - 11s - loss: 1.7407e-04 - val_loss: 1.8870e-04\n",
      "Epoch 15/50\n",
      "26550/26550 [==============================] - 11s - loss: 1.7258e-04 - val_loss: 1.8430e-04\n",
      "Epoch 16/50\n",
      "26550/26550 [==============================] - 11s - loss: 1.7150e-04 - val_loss: 1.8116e-04\n",
      "Epoch 17/50\n",
      "26550/26550 [==============================] - 11s - loss: 1.7035e-04 - val_loss: 1.7659e-04\n",
      "Epoch 18/50\n",
      "26550/26550 [==============================] - 11s - loss: 1.7044e-04 - val_loss: 1.8456e-04\n",
      "Epoch 19/50\n",
      "26550/26550 [==============================] - 11s - loss: 1.6990e-04 - val_loss: 1.7892e-04\n",
      "Epoch 20/50\n",
      "26550/26550 [==============================] - 11s - loss: 1.6820e-04 - val_loss: 1.7983e-04\n",
      "Epoch 21/50\n",
      "26550/26550 [==============================] - 11s - loss: 1.6789e-04 - val_loss: 1.7545e-04\n",
      "Epoch 22/50\n",
      "26550/26550 [==============================] - 11s - loss: 1.6743e-04 - val_loss: 1.7568e-04\n",
      "Epoch 23/50\n",
      "26550/26550 [==============================] - 11s - loss: 1.6769e-04 - val_loss: 1.7398e-04\n",
      "Epoch 24/50\n",
      "26550/26550 [==============================] - 11s - loss: 1.6535e-04 - val_loss: 1.7661e-04\n",
      "Epoch 25/50\n",
      "26550/26550 [==============================] - 11s - loss: 1.6437e-04 - val_loss: 1.6729e-04\n",
      "Epoch 26/50\n",
      "26550/26550 [==============================] - 11s - loss: 1.6482e-04 - val_loss: 1.7094e-04\n",
      "Epoch 27/50\n",
      "26550/26550 [==============================] - 11s - loss: 1.6473e-04 - val_loss: 1.7114e-04\n",
      "Epoch 28/50\n",
      "26550/26550 [==============================] - 11s - loss: 1.6410e-04 - val_loss: 1.7193e-04\n",
      "Epoch 29/50\n",
      "26550/26550 [==============================] - 11s - loss: 1.6373e-04 - val_loss: 1.6098e-04\n",
      "Epoch 30/50\n",
      "26550/26550 [==============================] - 11s - loss: 1.6313e-04 - val_loss: 1.6953e-04\n",
      "Epoch 31/50\n",
      "26550/26550 [==============================] - 11s - loss: 1.6449e-04 - val_loss: 1.7495e-04\n",
      "Epoch 32/50\n",
      "26550/26550 [==============================] - 11s - loss: 1.6264e-04 - val_loss: 1.7165e-04\n",
      "Epoch 33/50\n",
      "26550/26550 [==============================] - 11s - loss: 1.6301e-04 - val_loss: 1.7126e-04\n",
      "Epoch 34/50\n",
      "26550/26550 [==============================] - 11s - loss: 1.6224e-04 - val_loss: 1.6538e-04\n",
      "Epoch 35/50\n",
      "26550/26550 [==============================] - 11s - loss: 1.6190e-04 - val_loss: 1.6706e-04\n",
      "Epoch 00034: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x221a36f89e8>"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filepath=\"denoising_autoencoder_unsupervised_data.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=0, save_best_only=True, mode='min')\n",
    "early_stopping = EarlyStopping(monitor='val_loss', min_delta=pow(10, -8), patience=5, verbose=1, mode='auto')\n",
    "callbacks = [early_stopping, checkpoint]\n",
    "###############################################################################\n",
    "\n",
    "# train convolutional autoencoder\n",
    "autoencoder.fit(x_train_noisy, x_train,\n",
    "                epochs=50,\n",
    "                batch_size=64,\n",
    "                shuffle=True,\n",
    "                callbacks=callbacks,\n",
    "                validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Denoising MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11520000,)\n",
      "0\n",
      "Size of Data in memory is approximately: 1406.25 MB.\n"
     ]
    }
   ],
   "source": [
    "with open(path + \"pure_signal_extraction_data_indices.mem\", \"rb\") as input_file:\n",
    "    indices = pickle.load(input_file)\n",
    "\n",
    "# we will get a sinusoid estimation of an eeg signal and a noisy variant to train the autoencoder on\n",
    "shuffled_indices = []\n",
    "pure_data = []\n",
    "noisy_data = []\n",
    "\n",
    "\n",
    "for j in range(50):\n",
    "    # load twelve hours of data\n",
    "    dat, shuffled_indices = random_signal_loader(indices, shuffled_indices, path + \"pure_signal_extraction_data.mem\")\n",
    "    pure_data.extend(dat[1])\n",
    "    noisy_data.extend(dat[0])\n",
    "        \n",
    "# reshape into correctly size numpy arrays\n",
    "pure_data = np.array(pure_data).T\n",
    "noisy_data = np.array(noisy_data).T\n",
    "        \n",
    "# some stats\n",
    "print(pure_data.shape)\n",
    "print(len(shuffled_indices))\n",
    "print(\"Size of Data in memory is approximately: \" +\n",
    "          str(NUMBER_OF_TRAINING_INSTANCES*2*sys.getsizeof(pure_data[0])*pure_data.shape[0]/1048576) + \" MB.\")\n",
    "\n",
    "init_val = 2500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7603200,),\n",
       " (3916800,),\n",
       " (7603200,),\n",
       " (3916800,),\n",
       " numpy.ndarray,\n",
       " 7603200,\n",
       " 3916800,\n",
       " 0)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data dimensions\n",
    "input_dim = CHANNELS * TIMESTAMPS  # 14 timepoints of 14 channels (about 0.12 seconds of data)\n",
    "datapoint_dim = [1, CHANNELS * TIMESTAMPS]  # dimensionality of one datapoint, 128\n",
    "\n",
    "# stats\n",
    "n_train = int(pure_data.shape[0]*0.66) \n",
    "n_test = pure_data.shape[0] - n_train \n",
    "\n",
    "\n",
    "# non noisy\n",
    "x_train = set_creator(TIMESTAMPS, pure_data[:n_train])\n",
    "x_test = set_creator(TIMESTAMPS, pure_data[n_train:])\n",
    "\n",
    "# noisy\n",
    "x_train_noisy = set_creator(TIMESTAMPS, noisy_data[:n_train])\n",
    "x_test_noisy = set_creator(TIMESTAMPS, noisy_data[n_train:])\n",
    "\n",
    "# reworked stats\n",
    "n_train = x_train.shape[0]\n",
    "n_test = x_test.shape[0]\n",
    "\n",
    "x_train.shape, x_test.shape, x_train_noisy.shape, x_test_noisy.shape, type(x_train), n_train, n_test, n_train % TIMESTAMPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Deep autoencoder\n",
    "hidden_dim = 64  # For 280-dimensional input data (20*14) these layer sizes seem to suffice. Play with this if you change the input.\n",
    "encoding_dim = 32\n",
    "\n",
    "# this is our input placeholder\n",
    "input_eeg = Input(shape=(input_dim,))\n",
    "\n",
    "# \"encoded\" is the encoded representation of the input\n",
    "encoded = Dense(hidden_dim, activation=\"relu\")(input_eeg)\n",
    "encoded = Dense(hidden_dim, activation='relu')(encoded)\n",
    "encoded = Dense(encoding_dim, activation='relu')(encoded)\n",
    "\n",
    "# \"decoded\" is the lossy reconstruction of the input\n",
    "decoded = Dense(hidden_dim, activation='relu')(encoded)\n",
    "decoded = Dense(hidden_dim, activation='relu')(decoded)\n",
    "decoded = Dense(input_dim, activation='sigmoid')(decoded)\n",
    "\n",
    "# this model maps an input to its reconstruction\n",
    "autoencoder = Model(input_eeg, decoded)\n",
    "autoencoder.compile(optimizer='adam', loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# this model maps an input to its encoded representation\n",
    "encoder = Model(input_eeg, encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create a placeholder for an encoded input\n",
    "encoded_input = Input(shape=(encoding_dim,))\n",
    "\n",
    "# retrieve the decoder layers of the autoencoder model\n",
    "decoder_layer = autoencoder.layers[-3](encoded_input)\n",
    "decoder_layer = autoencoder.layers[-2](decoder_layer)\n",
    "decoder_layer = autoencoder.layers[-1](decoder_layer)\n",
    "\n",
    "# create the decoder model\n",
    "decoder = Model(encoded_input, decoder_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "total size of new array must be unchanged",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mValueError\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-64-efe6076bfc05>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mx_test_noisy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx_test_noisy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'float32'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mshift\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mx_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_train\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_dim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[0mx_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_test\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_dim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mx_train_noisy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx_train_noisy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_train\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_dim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: total size of new array must be unchanged"
     ]
    }
   ],
   "source": [
    "# We will normalize all values between 0 and 1 and we will reshape to (None, input_dim)\n",
    "shift = min(np.amin(x_train_noisy), np.amin(x_test_noisy), np.amin(x_train), np.amin(x_test))\n",
    "normalize = max(np.amax(x_train_noisy), np.amax(x_test_noisy), np.amax(x_train), np.amax(x_test)) - shift\n",
    "\n",
    "x_train = (x_train.astype('float32') - shift) / normalize\n",
    "x_train_noisy = (x_train_noisy.astype('float32') - shift) / normalize\n",
    "x_test = (x_test.astype('float32') - shift) / normalize\n",
    "x_test_noisy = (x_test_noisy.astype('float32') - shift) / normalize\n",
    "\n",
    "x_train = x_train.reshape(int(n_train/input_dim), input_dim)\n",
    "x_test = x_test.reshape(int(n_test/input_dim), input_dim)\n",
    "x_train_noisy = x_train_noisy.reshape(int(n_train/input_dim), input_dim)\n",
    "x_test_noisy = x_test_noisy.reshape(int(n_test/input_dim), input_dim)\n",
    "x_train.shape, x_test.shape, x_train_noisy.shape, x_test_noisy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "118800/118800 [==============================] - 2s - loss: 0.0026     \n",
      "Epoch 2/100\n",
      "118800/118800 [==============================] - 2s - loss: 7.0875e-05     \n",
      "Epoch 3/100\n",
      "118800/118800 [==============================] - 2s - loss: 5.1110e-05     \n",
      "Epoch 4/100\n",
      "118800/118800 [==============================] - 2s - loss: 3.8329e-05     \n",
      "Epoch 5/100\n",
      "118800/118800 [==============================] - 2s - loss: 3.3839e-05     \n",
      "Epoch 6/100\n",
      "118800/118800 [==============================] - 2s - loss: 3.0374e-05     \n",
      "Epoch 7/100\n",
      "118800/118800 [==============================] - 2s - loss: 2.6475e-05     \n",
      "Epoch 8/100\n",
      "118800/118800 [==============================] - 2s - loss: 3.6652e-05     \n",
      "Epoch 9/100\n",
      "118800/118800 [==============================] - 2s - loss: 2.3476e-05     \n",
      "Epoch 10/100\n",
      "118800/118800 [==============================] - 2s - loss: 2.5177e-05     \n",
      "Epoch 11/100\n",
      "118800/118800 [==============================] - 2s - loss: 2.4231e-05     \n",
      "Epoch 12/100\n",
      "118800/118800 [==============================] - 2s - loss: 2.3245e-05     \n",
      "Epoch 13/100\n",
      "118800/118800 [==============================] - 2s - loss: 2.1421e-05     \n",
      "Epoch 14/100\n",
      "118800/118800 [==============================] - 2s - loss: 2.3123e-05     \n",
      "Epoch 15/100\n",
      "118800/118800 [==============================] - 2s - loss: 2.2461e-05     \n",
      "Epoch 16/100\n",
      "118800/118800 [==============================] - 2s - loss: 2.1392e-05     \n",
      "Epoch 17/100\n",
      "118800/118800 [==============================] - 2s - loss: 2.2000e-05     \n",
      "Epoch 18/100\n",
      "118800/118800 [==============================] - 2s - loss: 2.1291e-05     \n",
      "Epoch 19/100\n",
      "118800/118800 [==============================] - 2s - loss: 2.0946e-05     \n",
      "Epoch 20/100\n",
      "118800/118800 [==============================] - 2s - loss: 2.1083e-05     \n",
      "Epoch 21/100\n",
      "118800/118800 [==============================] - 2s - loss: 2.0480e-05     \n",
      "Epoch 22/100\n",
      "118800/118800 [==============================] - 2s - loss: 2.0558e-05     \n",
      "Epoch 23/100\n",
      "118800/118800 [==============================] - 2s - loss: 2.0118e-05     \n",
      "Epoch 24/100\n",
      "118800/118800 [==============================] - 2s - loss: 2.0801e-05     \n",
      "Epoch 25/100\n",
      "118800/118800 [==============================] - 2s - loss: 1.9895e-05     \n",
      "Epoch 26/100\n",
      "118800/118800 [==============================] - 2s - loss: 1.9843e-05     \n",
      "Epoch 27/100\n",
      "118800/118800 [==============================] - 2s - loss: 1.9815e-05     \n",
      "Epoch 28/100\n",
      "118800/118800 [==============================] - 2s - loss: 1.9544e-05     \n",
      "Epoch 29/100\n",
      "118800/118800 [==============================] - 2s - loss: 2.0047e-05     \n",
      "Epoch 30/100\n",
      "118800/118800 [==============================] - 2s - loss: 1.9669e-05     \n",
      "Epoch 31/100\n",
      "118800/118800 [==============================] - 2s - loss: 1.9535e-05     \n",
      "Epoch 32/100\n",
      "118800/118800 [==============================] - 2s - loss: 1.9822e-05     \n",
      "Epoch 33/100\n",
      "118800/118800 [==============================] - 2s - loss: 1.8939e-05     \n",
      "Epoch 34/100\n",
      "118800/118800 [==============================] - 2s - loss: 1.9335e-05     \n",
      "Epoch 35/100\n",
      "118800/118800 [==============================] - 2s - loss: 1.8668e-05     \n",
      "Epoch 36/100\n",
      "118800/118800 [==============================] - 2s - loss: 1.9381e-05     \n",
      "Epoch 37/100\n",
      "118800/118800 [==============================] - 2s - loss: 1.8387e-05     \n",
      "Epoch 38/100\n",
      "118800/118800 [==============================] - 2s - loss: 1.8832e-05     \n",
      "Epoch 39/100\n",
      "118800/118800 [==============================] - 2s - loss: 1.9519e-05     \n",
      "Epoch 40/100\n",
      "118800/118800 [==============================] - 2s - loss: 1.8802e-05     \n",
      "Epoch 41/100\n",
      "118800/118800 [==============================] - 2s - loss: 1.9391e-05     \n",
      "Epoch 42/100\n",
      "118800/118800 [==============================] - 2s - loss: 1.8894e-05     \n",
      "Epoch 43/100\n",
      "118800/118800 [==============================] - 2s - loss: 1.8259e-05     \n",
      "Epoch 44/100\n",
      "118800/118800 [==============================] - 2s - loss: 1.8561e-05     \n",
      "Epoch 45/100\n",
      "118800/118800 [==============================] - 2s - loss: 1.9306e-05     \n",
      "Epoch 46/100\n",
      "118800/118800 [==============================] - 2s - loss: 1.8482e-05     \n",
      "Epoch 47/100\n",
      "118800/118800 [==============================] - 2s - loss: 1.8614e-05     \n",
      "Epoch 48/100\n",
      "118800/118800 [==============================] - 2s - loss: 1.8710e-05     \n",
      "Epoch 49/100\n",
      "118800/118800 [==============================] - 2s - loss: 1.8286e-05     \n",
      "Epoch 50/100\n",
      "118800/118800 [==============================] - 2s - loss: 1.8540e-05     \n",
      "Epoch 51/100\n",
      "118800/118800 [==============================] - 2s - loss: 1.8778e-05     \n",
      "Epoch 52/100\n",
      "118800/118800 [==============================] - 2s - loss: 1.8796e-05     \n",
      "Epoch 53/100\n",
      "118800/118800 [==============================] - 2s - loss: 1.7993e-05     \n",
      "Epoch 54/100\n",
      "118800/118800 [==============================] - 2s - loss: 1.8247e-05     \n",
      "Epoch 55/100\n",
      "118800/118800 [==============================] - 2s - loss: 1.8360e-05     \n",
      "Epoch 56/100\n",
      "118800/118800 [==============================] - 2s - loss: 1.8415e-05     \n",
      "Epoch 57/100\n",
      "118800/118800 [==============================] - 2s - loss: 1.8148e-05     \n",
      "Epoch 58/100\n",
      "118800/118800 [==============================] - 2s - loss: 1.8644e-05     \n",
      "Epoch 59/100\n",
      "118800/118800 [==============================] - 2s - loss: 1.8002e-05     \n",
      "Epoch 60/100\n",
      "118800/118800 [==============================] - 2s - loss: 1.8114e-05     \n",
      "Epoch 61/100\n",
      "118800/118800 [==============================] - 2s - loss: 1.8185e-05     \n",
      "Epoch 62/100\n",
      "118800/118800 [==============================] - 2s - loss: 1.8353e-05     \n",
      "Epoch 63/100\n",
      "118800/118800 [==============================] - 2s - loss: 1.9225e-05     \n",
      "Epoch 64/100\n",
      "118800/118800 [==============================] - 2s - loss: 1.7437e-05     \n",
      "Epoch 65/100\n",
      "118800/118800 [==============================] - 2s - loss: 1.7959e-05     \n",
      "Epoch 66/100\n",
      "118800/118800 [==============================] - 2s - loss: 1.8625e-05     \n",
      "Epoch 67/100\n",
      "118800/118800 [==============================] - 2s - loss: 1.7935e-05     \n",
      "Epoch 68/100\n",
      "118800/118800 [==============================] - 2s - loss: 1.8602e-05     \n",
      "Epoch 69/100\n",
      "118800/118800 [==============================] - 2s - loss: 1.7614e-05     \n",
      "Epoch 70/100\n",
      "118800/118800 [==============================] - 2s - loss: 1.7400e-05     \n",
      "Epoch 71/100\n",
      "118800/118800 [==============================] - 2s - loss: 1.8249e-05     \n",
      "Epoch 72/100\n",
      "118800/118800 [==============================] - 2s - loss: 1.8121e-05     \n",
      "Epoch 73/100\n",
      "118800/118800 [==============================] - 2s - loss: 1.7866e-05     \n",
      "Epoch 74/100\n",
      "118800/118800 [==============================] - 2s - loss: 1.7762e-05     \n",
      "Epoch 75/100\n",
      "118800/118800 [==============================] - 2s - loss: 1.7691e-05     \n",
      "Epoch 76/100\n",
      "118800/118800 [==============================] - 2s - loss: 1.8524e-05     \n",
      "Epoch 77/100\n",
      "118800/118800 [==============================] - 2s - loss: 1.7231e-05     \n",
      "Epoch 78/100\n",
      "118800/118800 [==============================] - 2s - loss: 1.8885e-05     \n",
      "Epoch 79/100\n",
      "118800/118800 [==============================] - 2s - loss: 1.7202e-05     \n",
      "Epoch 80/100\n",
      "118800/118800 [==============================] - 2s - loss: 1.8022e-05     \n",
      "Epoch 81/100\n",
      "118800/118800 [==============================] - 2s - loss: 1.8464e-05     \n",
      "Epoch 82/100\n",
      "118800/118800 [==============================] - 2s - loss: 1.7161e-05     \n",
      "Epoch 83/100\n",
      "118800/118800 [==============================] - 2s - loss: 1.7519e-05     \n",
      "Epoch 84/100\n",
      "118800/118800 [==============================] - 2s - loss: 1.7743e-05     \n",
      "Epoch 85/100\n",
      "118800/118800 [==============================] - 2s - loss: 1.7876e-05     \n",
      "Epoch 86/100\n",
      "118800/118800 [==============================] - 2s - loss: 1.8182e-05     \n",
      "Epoch 87/100\n",
      "118800/118800 [==============================] - 2s - loss: 1.7795e-05     \n",
      "Epoch 88/100\n",
      "118800/118800 [==============================] - 2s - loss: 1.7932e-05     \n",
      "Epoch 89/100\n",
      "118800/118800 [==============================] - 2s - loss: 1.7582e-05     \n",
      "Epoch 90/100\n",
      "118800/118800 [==============================] - 2s - loss: 1.7501e-05     \n",
      "Epoch 91/100\n",
      "118800/118800 [==============================] - 2s - loss: 1.7365e-05     \n",
      "Epoch 92/100\n",
      "118800/118800 [==============================] - 2s - loss: 1.7982e-05     \n",
      "Epoch 93/100\n",
      "118800/118800 [==============================] - 2s - loss: 1.7604e-05     \n",
      "Epoch 94/100\n",
      "118800/118800 [==============================] - 2s - loss: 1.7976e-05     \n",
      "Epoch 95/100\n",
      "118800/118800 [==============================] - 2s - loss: 1.7202e-05     \n",
      "Epoch 96/100\n",
      "118800/118800 [==============================] - 2s - loss: 1.7625e-05     \n",
      "Epoch 97/100\n",
      "118800/118800 [==============================] - 2s - loss: 1.7676e-05     \n",
      "Epoch 98/100\n",
      "118800/118800 [==============================] - 2s - loss: 1.7936e-05     \n",
      "Epoch 99/100\n",
      "118800/118800 [==============================] - 2s - loss: 1.7147e-05     \n",
      "Epoch 100/100\n",
      "118800/118800 [==============================] - 2s - loss: 1.7815e-05     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1dd71b27ac8>"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train convolutional autoencoder\n",
    "autoencoder.fit(x_train_noisy, x_train,\n",
    "                epochs=100,\n",
    "                batch_size=256,\n",
    "                shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Denoising autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1843200, 14)\n",
      "451\n",
      "Size of Data in memory is approximately: 225.0 MB.\n"
     ]
    }
   ],
   "source": [
    "with open(path + \"pure_signal_extraction_data_indices.mem\", \"rb\") as input_file:\n",
    "    indices = pickle.load(input_file)\n",
    "\n",
    "# we will get a sinusoid estimation of an eeg signal and a noisy variant to train the autoencoder on\n",
    "shuffled_indices = []\n",
    "pure_data = []\n",
    "noisy_data = []\n",
    "\n",
    "for i in range(14):\n",
    "    pure_data.append([])\n",
    "    noisy_data.append([])\n",
    "    for j in range(8):\n",
    "        # load four hours of data\n",
    "        dat, shuffled_indices = random_signal_loader(indices, shuffled_indices, path + \"pure_signal_extraction_data.mem\")\n",
    "        pure_data[i].extend(dat[1])\n",
    "        noisy_data[i].extend(dat[0])\n",
    "        \n",
    "# reshape into correctly size numpy arrays\n",
    "pure_data = np.array(pure_data).T\n",
    "noisy_data = np.array(noisy_data).T\n",
    "        \n",
    "# some stats\n",
    "print(pure_data.shape)\n",
    "print(len(shuffled_indices))\n",
    "print(\"Size of Data in memory is approximately: \" +\n",
    "          str(NUMBER_OF_TRAINING_INSTANCES*2*sys.getsizeof(pure_data[0][0])*pure_data.shape[0]/1048576) + \" MB.\")\n",
    "\n",
    "init_val = 2500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1216512, 14),\n",
       " (626688, 14),\n",
       " (1216512, 14),\n",
       " (626688, 14),\n",
       " numpy.ndarray,\n",
       " 1216512,\n",
       " 626688,\n",
       " 0)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data dimensions\n",
    "input_dim = CHANNELS * TIMESTAMPS  # 14 timepoints of 14 channels (about 0.12 seconds of data)\n",
    "datapoint_dim = [1,TIMESTAMPS, CHANNELS,1]  # dimensionality of one datapoint\n",
    "\n",
    "# stats\n",
    "n_train = int(pure_data.shape[0]*0.66) \n",
    "n_test = pure_data.shape[0] - n_train \n",
    "\n",
    "\n",
    "# non noisy\n",
    "x_train = set_creator(TIMESTAMPS, pure_data[:n_train, :])\n",
    "x_test = set_creator(TIMESTAMPS, pure_data[n_train:, :])\n",
    "\n",
    "# noisy\n",
    "x_train_noisy = set_creator(TIMESTAMPS, noisy_data[:n_train, :])\n",
    "x_test_noisy = set_creator(TIMESTAMPS, noisy_data[n_train:, :])\n",
    "\n",
    "# reworked stats\n",
    "n_train = x_train.shape[0]\n",
    "n_test = x_test.shape[0]\n",
    "\n",
    "x_train.shape, x_test.shape, x_train_noisy.shape, x_test_noisy.shape, type(x_train), n_train, n_test, n_train % TIMESTAMPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Convolutional autoencoder\n",
    "input_img = Input(shape=(TIMESTAMPS, CHANNELS, 1))  # The data will now be treated as a 14 by 14 image \n",
    "\n",
    "x = Conv2D(32, (3, 3), activation='relu', padding='same')(input_img)\n",
    "x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "x = MaxPooling2D((2, 1), padding='same')(x)\n",
    "x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "encoded = MaxPooling2D((2, 1), padding='same')(x)\n",
    "\n",
    "# at this point the representation is (16, 4, 32) i.e. lot-dimensional\n",
    "\n",
    "x = Conv2D(32, (3, 3), activation='relu', padding=\"same\")(encoded)\n",
    "x = UpSampling2D((2, 1))(x)\n",
    "x = Conv2D(32, (3, 3), activation='relu', padding=\"same\")(x)\n",
    "x = UpSampling2D((2, 1))(x)\n",
    "x = Conv2D(32, (3, 3), activation='relu', padding=\"same\")(x)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "x = Conv2D(32, (1, 2), activation='relu')(x)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "decoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n",
    "\n",
    "\n",
    "# The autoencoder\n",
    "autoencoder = Model(input_img, decoded)\n",
    "autoencoder.compile(optimizer='adam', loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this model maps an input to its encoded representation\n",
    "encoder = Model(input_img, encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a placeholder for an encoded input\n",
    "encoded_input = Input(shape=(16,4,32))\n",
    "\n",
    "# retrieve the decoder layers of the autoencoder model\n",
    "decoder_layer = autoencoder.layers[-9](encoded_input)\n",
    "decoder_layer = autoencoder.layers[-8](decoder_layer)\n",
    "decoder_layer = autoencoder.layers[-7](decoder_layer)\n",
    "decoder_layer = autoencoder.layers[-6](decoder_layer)\n",
    "decoder_layer = autoencoder.layers[-5](decoder_layer)\n",
    "decoder_layer = autoencoder.layers[-4](decoder_layer)\n",
    "decoder_layer = autoencoder.layers[-3](decoder_layer)\n",
    "decoder_layer = autoencoder.layers[-2](decoder_layer)\n",
    "decoder_layer = autoencoder.layers[-1](decoder_layer)\n",
    "\n",
    "# create the decoder model\n",
    "decoder = Model(encoded_input, decoder_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((118800, 64, 1, 1), (61200, 64, 1, 1), (118800, 64, 1, 1), (61200, 64, 1, 1))"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We will normalize all values between 0 and 1 and we will reshape to (None, 14, 14, 1), 14 timepoints with 14 signals\n",
    "shift = min(np.amin(x_train_noisy), np.amin(x_test_noisy), np.amin(x_train), np.amin(x_test))\n",
    "normalize = max(np.amax(x_train_noisy), np.amax(x_test_noisy), np.amax(x_train), np.amax(x_test)) - shift\n",
    "\n",
    "x_train = (x_train.astype('float32') - shift) / normalize\n",
    "x_train_noisy = (x_train_noisy.astype('float32') - shift) / normalize\n",
    "x_test = (x_test.astype('float32') - shift) / normalize\n",
    "x_test_noisy = (x_test_noisy.astype('float32') - shift) / normalize\n",
    "\n",
    "x_train = x_train.reshape(int(n_train/TIMESTAMPS), TIMESTAMPS, CHANNELS, 1)\n",
    "x_test = x_test.reshape(int(n_test/TIMESTAMPS), TIMESTAMPS, CHANNELS, 1)\n",
    "x_train_noisy = x_train_noisy.reshape(int(n_train/TIMESTAMPS), TIMESTAMPS, CHANNELS, 1)\n",
    "x_test_noisy = x_test_noisy.reshape(int(n_test/TIMESTAMPS), TIMESTAMPS, CHANNELS, 1)\n",
    "x_train.shape, x_test.shape, x_train_noisy.shape, x_test_noisy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "4752/4752 [==============================] - 6s - loss: 0.0015     \n",
      "Epoch 2/50\n",
      "4752/4752 [==============================] - 6s - loss: 0.0015     \n",
      "Epoch 3/50\n",
      "4752/4752 [==============================] - 6s - loss: 0.0015     \n",
      "Epoch 4/50\n",
      "4752/4752 [==============================] - 6s - loss: 0.0015     \n",
      "Epoch 5/50\n",
      "4752/4752 [==============================] - 6s - loss: 0.0015     \n",
      "Epoch 6/50\n",
      "4752/4752 [==============================] - 6s - loss: 0.0015     \n",
      "Epoch 7/50\n",
      "4752/4752 [==============================] - 6s - loss: 0.0015     \n",
      "Epoch 8/50\n",
      "4752/4752 [==============================] - 6s - loss: 0.0015     \n",
      "Epoch 9/50\n",
      "4752/4752 [==============================] - 6s - loss: 0.0015     \n",
      "Epoch 10/50\n",
      "4752/4752 [==============================] - 6s - loss: 0.0015     \n",
      "Epoch 11/50\n",
      "4752/4752 [==============================] - 6s - loss: 0.0015     \n",
      "Epoch 12/50\n",
      "4752/4752 [==============================] - 6s - loss: 0.0014     \n",
      "Epoch 13/50\n",
      "4752/4752 [==============================] - 6s - loss: 0.0014     \n",
      "Epoch 14/50\n",
      "4752/4752 [==============================] - 6s - loss: 0.0014     \n",
      "Epoch 15/50\n",
      "4752/4752 [==============================] - 6s - loss: 0.0014     \n",
      "Epoch 16/50\n",
      "4752/4752 [==============================] - 6s - loss: 0.0014     \n",
      "Epoch 17/50\n",
      "4752/4752 [==============================] - 6s - loss: 0.0014     \n",
      "Epoch 18/50\n",
      "4752/4752 [==============================] - 6s - loss: 0.0014     \n",
      "Epoch 19/50\n",
      "4752/4752 [==============================] - 6s - loss: 0.0014     \n",
      "Epoch 20/50\n",
      "4752/4752 [==============================] - 6s - loss: 0.0014     \n",
      "Epoch 21/50\n",
      "4752/4752 [==============================] - 6s - loss: 0.0014     \n",
      "Epoch 22/50\n",
      "4752/4752 [==============================] - 6s - loss: 0.0014     \n",
      "Epoch 23/50\n",
      "4752/4752 [==============================] - 6s - loss: 0.0014     \n",
      "Epoch 24/50\n",
      "4752/4752 [==============================] - 6s - loss: 0.0014     \n",
      "Epoch 25/50\n",
      "4752/4752 [==============================] - 6s - loss: 0.0014     \n",
      "Epoch 26/50\n",
      "4752/4752 [==============================] - 6s - loss: 0.0014     \n",
      "Epoch 27/50\n",
      "4752/4752 [==============================] - 6s - loss: 0.0014     \n",
      "Epoch 28/50\n",
      "4752/4752 [==============================] - 6s - loss: 0.0014     \n",
      "Epoch 29/50\n",
      "4752/4752 [==============================] - 6s - loss: 0.0014     \n",
      "Epoch 30/50\n",
      " 256/4752 [>.............................] - ETA: 6s - loss: 0.0014"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-125-0c442f355f9f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m                 \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m                 \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m                 shuffle=True)\n\u001b[0m",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[0;32m   1505\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1506\u001b[0m                               \u001b[0mcallback_metrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1507\u001b[1;33m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1508\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[1;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\u001b[0m\n\u001b[0;32m   1154\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'size'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1155\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1156\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1157\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1158\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2267\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[0;32m   2268\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2269\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2270\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2271\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    776\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 778\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    779\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    980\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 982\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    983\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1030\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1032\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1033\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1037\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1040\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[0;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1021\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1022\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1023\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train convolutional autoencoder\n",
    "autoencoder.fit(x_train_noisy, x_train,\n",
    "                epochs=50,\n",
    "                batch_size=64,\n",
    "                shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3277, 64, 14, 1)\n",
      "(3277, 64, 14, 1)\n",
      "(3277, 16, 7, 16)\n"
     ]
    }
   ],
   "source": [
    "# encode and decode \n",
    "encoded_eegs = encoder.predict(x_test_noisy)\n",
    "decoded_eegs = decoder.predict(encoded_eegs)\n",
    "\n",
    "# these should have the same shape\n",
    "print(x_test_noisy.shape)\n",
    "print(decoded_eegs.shape)\n",
    "\n",
    "# this should have the correct encoded dimensions\n",
    "print(encoded_eegs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import correlation, euclidean, mahalanobis, minkowski,cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corr: 0.703949048631\n",
      "euc: 1.1683573722839355\n",
      "cos: 0.000456028240879\n",
      "cheb: 0.0991001\n"
     ]
    }
   ],
   "source": [
    "# Get a sample from the test set\n",
    "sample = x_test_noisy[10].reshape(datapoint_dim)\n",
    "encoded_x = encoder.predict(sample)\n",
    "decoded_x = decoder.predict(encoded_x)\n",
    "\n",
    "# Apply different distance measures in order to evaluate the decoding (extend this for grid search) ()\n",
    "test_x = sample.reshape(input_dim)\n",
    "test_y = decoded_x.reshape(input_dim)\n",
    "print(\"corr: \" + str(correlation(test_x,test_y)))\n",
    "print(\"euc: \" + str(euclidean(test_x,test_y)))\n",
    "print(\"cos: \" + str(cosine(test_x,test_y))) # probably the best measure due to the high dimensionality of the data\n",
    "print(\"cheb: \" + str(minkowski(test_x,test_y,np.inf))) # chebyshev distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEACAYAAACtVTGuAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBEEQhCX/D+WXqxJdXnLtAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x22140026080>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Draw a plot to compare a slice of the test signal and its decoded reconstruction\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "x_test_noisy = x_test_noisy.reshape(n_test, CHANNELS)\n",
    "x_test = x_test.reshape(n_test, CHANNELS)\n",
    "decoded_eegs = decoded_eegs.reshape((n_test, CHANNELS))\n",
    "# plt.plot(noisy_data[999990:1000200], label=\"noisy\")\n",
    "plt.plot(x_test[:100000,2], label=\"pure\")\n",
    "plt.plot(decoded_eegs[:100000,3], label = \"reconstructed\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
